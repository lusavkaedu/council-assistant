{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dbb1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 72 divisions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning divisions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72/72 [17:43<00:00, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished scraping. Saved to ../data/jsons/kent_candidate_profiles.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "MOTHER_URL = \"https://whocanivotefor.co.uk/elections/local.kent.2025-05-01/kent-local-election/\"\n",
    "BASE_URL = \"https://whocanivotefor.co.uk\"\n",
    "OUTPUT_JSONL = \"../data/jsons/kent_candidate_profiles.jsonl\"\n",
    "\n",
    "# Utilities\n",
    "def load_existing_jsonl(path):\n",
    "    seen = set()\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    seen.add(data.get(\"profile_url\"))\n",
    "                except:\n",
    "                    continue\n",
    "    return seen\n",
    "\n",
    "def save_jsonl_entry(path, data):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Step 1: Get all child division links from the mother page\n",
    "def get_child_division_urls(mother_url):\n",
    "    res = requests.get(mother_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\"a[href^='/elections/local.kent']\")\n",
    "    division_urls = {urljoin(BASE_URL, a[\"href\"]) for a in links if a[\"href\"].count(\"/\") > 3}\n",
    "    return sorted(division_urls)\n",
    "\n",
    "# Step 2: Extract all candidate profile URLs from a division page\n",
    "def get_candidate_urls(division_url):\n",
    "    res = requests.get(division_url)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.select(\"a[href^='/person/']\")\n",
    "    return [urljoin(BASE_URL, a[\"href\"]) for a in links]\n",
    "\n",
    "# Step 3: Parse the candidate profile page\n",
    "def scrape_candidate_profile(url):\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    name_tag = soup.select_one(\"h2.ds-candidate-name\")\n",
    "    if not name_tag:\n",
    "        return None\n",
    "\n",
    "    profile = {\n",
    "        \"profile_url\": url,\n",
    "        \"name\": name_tag.text.strip(),\n",
    "        \"photo_url\": None,\n",
    "        \"party\": None,\n",
    "        \"ward\": None,\n",
    "        \"votes\": None,\n",
    "        \"position\": None,\n",
    "        \"statement\": None,\n",
    "        \"email\": None,\n",
    "        \"social_links\": {},\n",
    "        \"elections\": []\n",
    "    }\n",
    "\n",
    "    # Image\n",
    "    img_tag = soup.select_one(\"img[alt*='profile photo']\")\n",
    "    if img_tag:\n",
    "        profile[\"photo_url\"] = img_tag[\"src\"]\n",
    "\n",
    "    # Paragraph with election summary\n",
    "    p_tag = soup.select_one(\"section.ds-candidate p\")\n",
    "    if p_tag:\n",
    "        profile[\"summary\"] = p_tag.text.strip()\n",
    "\n",
    "    # Statement\n",
    "    quote_block = soup.select_one(\"blockquote\")\n",
    "    if quote_block:\n",
    "        profile[\"statement\"] = quote_block.get_text(\" \", strip=True)\n",
    "\n",
    "    # Email\n",
    "    email_tag = soup.select_one(\"a[href^='mailto:']\")\n",
    "    if email_tag:\n",
    "        profile[\"email\"] = email_tag.text.strip()\n",
    "\n",
    "    # Social and external links\n",
    "    for dt in soup.select(\"dl.ds-descriptions dt\"):\n",
    "        label = dt.text.strip().lower()\n",
    "        dd = dt.find_next_sibling(\"div\") or dt.find_next_sibling(\"dd\")\n",
    "        if dd and dd.find(\"a\"):\n",
    "            profile[\"social_links\"][label] = dd.find(\"a\").get(\"href\")\n",
    "\n",
    "    # Election table\n",
    "    table = soup.select_one(\"table\")\n",
    "    if table:\n",
    "        for row in table.select(\"tr\")[1:]:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 5:\n",
    "                profile[\"elections\"].append({\n",
    "                    \"year\": cols[0].text.strip(),\n",
    "                    \"election\": cols[1].text.strip(),\n",
    "                    \"party\": cols[2].text.strip(),\n",
    "                    \"result\": cols[3].text.strip(),\n",
    "                    \"position\": cols[4].text.strip()\n",
    "                })\n",
    "\n",
    "    return profile\n",
    "\n",
    "# MAIN SCRIPT\n",
    "def main():\n",
    "    seen_profiles = load_existing_jsonl(OUTPUT_JSONL)\n",
    "    child_urls = get_child_division_urls(MOTHER_URL)\n",
    "    print(f\"üîç Found {len(child_urls)} divisions\")\n",
    "\n",
    "    for division_url in tqdm(child_urls, desc=\"Scanning divisions\"):\n",
    "        try:\n",
    "            candidate_urls = get_candidate_urls(division_url)\n",
    "            for candidate_url in candidate_urls:\n",
    "                if candidate_url in seen_profiles:\n",
    "                    continue\n",
    "                data = scrape_candidate_profile(candidate_url)\n",
    "                if data:\n",
    "                    save_jsonl_entry(OUTPUT_JSONL, data)\n",
    "                    seen_profiles.add(candidate_url)\n",
    "                time.sleep(2)  # Throttle\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in {division_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"‚úÖ Finished scraping. Saved to {OUTPUT_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695418f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
