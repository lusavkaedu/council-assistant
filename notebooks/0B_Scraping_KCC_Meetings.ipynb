{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccedcfa",
   "metadata": {},
   "source": [
    "# Scraping KCC Meetings in bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ad491",
   "metadata": {},
   "source": [
    "### Scraping all meetings from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e746c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping meetings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [1:00:58<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "OUTPUT_FILE = \"../data/meetings/meetings_metadata.jsonl\"\n",
    "COMMITTEE_ID = \"144\"\n",
    "MIDS = range(8000, 9000)  # Change as needed\n",
    "BASE_URL = \"https://democracy.kent.gov.uk\"\n",
    "\n",
    "def clean_day_suffix(date_str):\n",
    "    return re.sub(r'(\\d{1,2})(st|nd|rd|th)', r'\\1', date_str)\n",
    "\n",
    "def load_seen_ids(output_file):\n",
    "    seen = set()\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    seen.add(obj[\"web_meeting_code\"])\n",
    "                except:\n",
    "                    continue\n",
    "    return seen\n",
    "\n",
    "def scrape_meeting_metadata(mid, cid=\"144\"):\n",
    "    url = f\"{BASE_URL}/ieListDocuments.aspx?CId={cid}&MId={mid}\"\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, timeout=6)\n",
    "        if res.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # --- Status detection ---\n",
    "        page_title = soup.find(\"title\").text.strip()\n",
    "        status_match = re.search(r'\\b(CANCELLED|WITHDRAWN|POSTPONED|MOVED|NEW)\\b', page_title.upper())\n",
    "        status = status_match.group(1).lower() if status_match else \"scheduled\"\n",
    "\n",
    "        # --- Heading and metadata ---\n",
    "        title_candidates = soup.find_all([\"h1\", \"h2\"])\n",
    "        full_heading = \"\"\n",
    "        for tag in title_candidates:\n",
    "            if \"Committee\" in tag.get_text() or re.search(r\"\\d{4}\", tag.get_text()):\n",
    "                full_heading = tag.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "        match = re.match(\n",
    "            r\"^(.*?)\\s*-\\s*(Monday|Tuesday|Wednesday|Thursday|Friday),\\s*(.*?),\\s*(\\d{4})\\s*(\\d{1,2}\\.\\d{2})\\s*(am|pm)\",\n",
    "            full_heading\n",
    "        )\n",
    "\n",
    "        if match:\n",
    "            committee_name = match.group(1).strip()\n",
    "            raw_day = clean_day_suffix(match.group(3))\n",
    "            meeting_date = datetime.strptime(f\"{raw_day}, {match.group(4)}\", \"%d %B, %Y\").strftime(\"%Y-%m-%d\")\n",
    "            meeting_time = datetime.strptime(match.group(5) + match.group(6), \"%I.%M%p\").strftime(\"%H:%M\")\n",
    "        else:\n",
    "            committee_name = None\n",
    "            meeting_date = None\n",
    "            meeting_time = None\n",
    "\n",
    "        # --- Agenda item extraction with PDFs ---\n",
    "        agenda_items = []\n",
    "        for row in soup.find_all(\"tr\"):\n",
    "            number_cell = row.find(\"td\", class_=\"mgItemNumberCell\")\n",
    "            content_cells = row.find_all(\"td\")\n",
    "            if number_cell and len(content_cells) > 1:\n",
    "                item_number = number_cell.get_text(strip=True)\n",
    "                content_td = content_cells[1]\n",
    "                paragraphs = content_td.find_all(\"p\")\n",
    "                item_title = paragraphs[0].get_text(strip=True) if paragraphs else \"\"\n",
    "                item_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs[1:]) if len(paragraphs) > 1 else \"\"\n",
    "\n",
    "                # Find any PDF links inside this agenda item\n",
    "                item_pdfs = []\n",
    "                for a in content_td.find_all(\"a\", href=True):\n",
    "                    href = a[\"href\"]\n",
    "                    if href.lower().endswith(\".pdf\"):\n",
    "                        full_url = BASE_URL + \"/\" + href.lstrip(\"/\")\n",
    "                        item_pdfs.append(full_url)\n",
    "\n",
    "                agenda_items.append({\n",
    "                    \"item_number\": item_number,\n",
    "                    \"item_title\": item_title,\n",
    "                    \"item_text\": item_text,\n",
    "                    \"pdf_urls\": item_pdfs\n",
    "                })\n",
    "\n",
    "        return {\n",
    "            \"web_meeting_code\": str(mid),\n",
    "            \"meeting_title\": full_heading,\n",
    "            \"meeting_status\": status,\n",
    "            \"committee_name\": committee_name,\n",
    "            \"meeting_date\": meeting_date,\n",
    "            \"meeting_time\": meeting_time,\n",
    "            \"agenda_items\": agenda_items\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"web_meeting_code\": str(mid), \"error\": str(e)}\n",
    "\n",
    "def run_scrape_batch(mids, cid, output_path, delay=(1.5, 3.5)):\n",
    "    seen_ids = load_seen_ids(output_path)\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for mid in tqdm(mids, desc=\"Scraping meetings\"):\n",
    "            if str(mid) in seen_ids:\n",
    "                continue\n",
    "            data = scrape_meeting_metadata(mid, cid)\n",
    "            if data:\n",
    "                f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "            time.sleep(random.uniform(*delay))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scrape_batch(MIDS, COMMITTEE_ID, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ec4e5d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9873 entries, 0 to 9872\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   meeting_code  9873 non-null   object\n",
      " 1   item_number   9873 non-null   object\n",
      " 2   item_title    9873 non-null   object\n",
      " 3   text          9873 non-null   object\n",
      " 4   word_count    9873 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 385.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the scraped JSONL file\n",
    "jsonl_path = \"../data/meetings/meetings_metadata.jsonl\"\n",
    "meetings = []\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            meetings.append(json.loads(line))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Flatten agenda items\n",
    "flattened = []\n",
    "for meeting in meetings:\n",
    "    meeting_code = meeting.get(\"web_meeting_code\", \"\")\n",
    "    agenda_items = meeting.get(\"agenda_items\", [])\n",
    "    for item in agenda_items:\n",
    "        item_text = item.get(\"item_text\", \"\").strip()\n",
    "        flattened.append({\n",
    "            \"meeting_code\": meeting_code,\n",
    "            \"item_number\": item.get(\"item_number\", \"\"),\n",
    "            \"item_title\": item.get(\"item_title\", \"\"),\n",
    "            \"text\": item_text,\n",
    "            \"word_count\": len(item_text.split())\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for review\n",
    "df_agenda = pd.DataFrame(flattened)\n",
    "\n",
    "# Display to user\n",
    "df_agenda.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4aa9ae",
   "metadata": {},
   "source": [
    "### Saving agenda items in chunks warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2647b",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Clean and Export All Agenda Chunks\n",
    "\n",
    "This code block loads the full council meeting metadata from `meetings_metadata.jsonl`, and generates a list of agenda chunks.\n",
    "\n",
    "Each chunk includes:\n",
    "- `chunk_id` (safe and unique)\n",
    "- `meeting_code`, `meeting_date`, `committee_name`\n",
    "- `item_number`, `item_title`, and full `text`\n",
    "- `word_count` for reference\n",
    "\n",
    "The output is saved to: \n",
    "data/chunks/minutes_cleaned/chunks.jsonl\n",
    "\n",
    "\n",
    "This cleaned file can be used as the input for embedding or RAG indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25df47d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/chunks/minutes/chunks.jsonl'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import missing regex module and re-run the cleaned chunk generation\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "input_path = Path(\"../data/meetings/meetings_metadata.jsonl\")\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meetings = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Define output path again\n",
    "output_folder = Path(\"../data/chunks/minutes/\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_folder / \"chunks.jsonl\"\n",
    "\n",
    "# Keywords to filter out low-value items\n",
    "low_signal_keywords = [\n",
    "    \"apologies\", \"substitutes\", \"panel business\",\n",
    "    \"motion to exclude\", \"minutes of the meeting\",\n",
    "    \"future work programme\", \"webcast\", \"any other business\"\n",
    "]\n",
    "\n",
    "# Prepare cleaned chunk records\n",
    "cleaned_chunks = []\n",
    "for meeting in meetings:\n",
    "    meeting_code = meeting.get(\"web_meeting_code\", \"\")\n",
    "    meeting_date = meeting.get(\"meeting_date\")\n",
    "    committee_name = meeting.get(\"committee_name\")\n",
    "    agenda_items = meeting.get(\"agenda_items\", [])\n",
    "\n",
    "    for idx, item in enumerate(agenda_items):\n",
    "        item_number = item.get(\"item_number\", \"\").strip()\n",
    "        item_title = item.get(\"item_title\", \"\").strip()\n",
    "        item_text = item.get(\"item_text\", \"\").strip()\n",
    "        word_count = len(item_text.split())\n",
    "\n",
    "        # Safe chunk_id\n",
    "        base_id = item_number if item_number else f\"item{idx}\"\n",
    "        clean_id = re.sub(r\"[^\\w]+\", \"\", base_id.upper()) or f\"ID{hashlib.md5(item_title.encode()).hexdigest()[:6]}\"\n",
    "        chunk_id = f\"{meeting_code}_{clean_id}\"\n",
    "\n",
    "        cleaned_chunks.append({\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"meeting_code\": meeting_code,\n",
    "            \"meeting_date\": meeting_date,\n",
    "            \"committee_name\": committee_name,\n",
    "            \"item_number\": item_number,\n",
    "            \"item_title\": item_title,\n",
    "            \"text\": item_text,\n",
    "            \"word_count\": word_count\n",
    "        })\n",
    "\n",
    "# Save cleaned chunks to JSONL\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in cleaned_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "output_path.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd904e",
   "metadata": {},
   "source": [
    "### Filtering out meaningless chunks for embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7238af",
   "metadata": {},
   "source": [
    "this is done in a script 4a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e8e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eee93ad",
   "metadata": {},
   "source": [
    "### Scraping 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = scrape_meeting_metadata(mid=9502, cid=\"144\")\n",
    "test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
