{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccedcfa",
   "metadata": {},
   "source": [
    "# Scraping KCC Cabinet Issues and Decisions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee93ad",
   "metadata": {},
   "source": [
    "### Scraping Level 1 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49558c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Level 1 plans: 100%|██████████| 3/3 [00:12<00:00,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Results saved to ../data/jsons/level1_plans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "BASE_URL = \"https://democracy.kent.gov.uk:9071\"\n",
    "OUTPUT_JSONL = \"../data/jsons/level1_plans.jsonl\"\n",
    "INPUT_URLS = [\n",
    "    \"https://democracy.kent.gov.uk:9071/mgListPlanItems.aspx?PlanId=632&RP=115\",\n",
    "    \"https://democracy.kent.gov.uk:9071/mgListPlanItems.aspx?PlanId=629&RP=115\",\n",
    "    \"https://democracy.kent.gov.uk:9071/mgListPlanItems.aspx?PlanId=628&RP=115\"\n",
    "]\n",
    "\n",
    "def extract_plan_id(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parse_qs(parsed.query).get(\"PlanId\", [None])[0]\n",
    "\n",
    "def extract_plan_metadata(soup):\n",
    "    meta_title = soup.find(\"meta\", attrs={\"name\": \"DC.title\"})\n",
    "    meta_date = soup.find(\"meta\", attrs={\"name\": \"DC.date\"})\n",
    "    return (\n",
    "        meta_title[\"content\"].strip() if meta_title else \"Unknown Title\",\n",
    "        meta_date[\"content\"] if meta_date else None\n",
    "    )\n",
    "\n",
    "def parse_issue_row(row):\n",
    "    issue = {\n",
    "        \"decision_code\": None,\n",
    "        \"title\": None,\n",
    "        \"url\": None,\n",
    "        \"decision_maker\": None,\n",
    "        \"decision_due\": None,\n",
    "        \"lead_officer\": None,\n",
    "        \"status\": None,\n",
    "        \"first_published\": None,\n",
    "        \"restriction\": None\n",
    "    }\n",
    "\n",
    "    # Find the link to the issue history (Level 2)\n",
    "    title_link = row.select_one(\"a[href*='mgIssueHistoryHome.aspx']\")\n",
    "    if title_link:\n",
    "        issue[\"title\"] = title_link.text.strip()\n",
    "        issue[\"url\"] = urljoin(BASE_URL, title_link[\"href\"])\n",
    "        # Try to find the decision code before the title link\n",
    "        preceding_span = title_link.find_previous(\"span\")\n",
    "        if preceding_span:\n",
    "            code = preceding_span.get_text(strip=True)\n",
    "            if \"/\" in code or \"-\" in code:\n",
    "                issue[\"decision_code\"] = code\n",
    "\n",
    "    # Now extract key-value pairs from ALL text lines inside the row\n",
    "    raw_lines = row.get_text(separator=\"\\n\", strip=True).splitlines()\n",
    "    for line in raw_lines:\n",
    "        line = line.strip()\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, val = line.split(\":\", 1)\n",
    "        key = key.lower().strip()\n",
    "        val = val.strip()\n",
    "\n",
    "        if \"decision maker\" in key:\n",
    "            issue[\"decision_maker\"] = val\n",
    "        elif key.startswith(\"decision\"):\n",
    "            issue[\"decision_due\"] = val\n",
    "        elif \"lead officer\" in key:\n",
    "            issue[\"lead_officer\"] = val\n",
    "        elif \"status\" in key:\n",
    "            issue[\"status\"] = val\n",
    "        elif \"first published\" in key:\n",
    "            issue[\"first_published\"] = val\n",
    "        elif \"restriction\" in key:\n",
    "            issue[\"restriction\"] = val\n",
    "\n",
    "    return issue\n",
    "\n",
    "def scrape_plan_page(url):\n",
    "    res = requests.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    plan_id = extract_plan_id(url)\n",
    "    plan_title, plan_date = extract_plan_metadata(soup)\n",
    "\n",
    "    issues = []\n",
    "\n",
    "    # Anchor around all decision links to Level 2\n",
    "    for anchor in soup.select(\"a[href*='mgIssueHistoryHome.aspx']\"):\n",
    "        issue = {\n",
    "            \"decision_code\": None,\n",
    "            \"title\": None,\n",
    "            \"url\": None,\n",
    "            \"decision_maker\": None,\n",
    "            \"decision_due\": None,\n",
    "            \"lead_officer\": None,\n",
    "            \"status\": None,\n",
    "            \"first_published\": None,\n",
    "            \"restriction\": None\n",
    "        }\n",
    "\n",
    "        full_title = anchor.get_text(strip=True)\n",
    "        issue[\"title\"] = full_title\n",
    "\n",
    "        # Extract decision_code directly from the title\n",
    "        match = re.match(r\"(\\d{2}/\\d{5})\", full_title)\n",
    "        if match:\n",
    "            issue[\"decision_code\"] = match.group(1)\n",
    "\n",
    "        issue[\"url\"] = urljoin(BASE_URL, anchor[\"href\"])\n",
    "\n",
    "        # Walk through surrounding text nodes and paragraphs\n",
    "        context_block = []\n",
    "        parent = anchor.find_parent()\n",
    "        while parent and len(context_block) < 12:\n",
    "            ps = parent.find_all(\"p\")\n",
    "            if ps:\n",
    "                for p in ps:\n",
    "                    text = p.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        context_block.append(text)\n",
    "                break\n",
    "            parent = parent.find_parent()\n",
    "\n",
    "        # Parse key-value fields from block\n",
    "        for line in context_block:\n",
    "            if \":\" not in line:\n",
    "                continue\n",
    "            key, val = line.split(\":\", 1)\n",
    "            key = key.lower().strip()\n",
    "            val = val.strip()\n",
    "\n",
    "            if \"decision maker\" in key:\n",
    "                issue[\"decision_maker\"] = val\n",
    "            elif key.startswith(\"decision\"):\n",
    "                issue[\"decision_due\"] = val\n",
    "            elif \"lead officer\" in key:\n",
    "                issue[\"lead_officer\"] = val\n",
    "            elif \"status\" in key:\n",
    "                issue[\"status\"] = val\n",
    "            elif \"first published\" in key:\n",
    "                issue[\"first_published\"] = val\n",
    "            elif \"restriction\" in key:\n",
    "                issue[\"restriction\"] = val\n",
    "\n",
    "        # Only add if title and URL are found\n",
    "        if issue[\"title\"] and issue[\"url\"]:\n",
    "            issues.append(issue)\n",
    "\n",
    "    return {\n",
    "        \"plan_id\": plan_id,\n",
    "        \"plan_title\": plan_title,\n",
    "        \"plan_date\": plan_date,\n",
    "        \"plan_url\": url,\n",
    "        \"issues\": issues\n",
    "    }\n",
    "\n",
    "    # Append last block\n",
    "    if current_issue:\n",
    "        issues.append(current_issue)\n",
    "\n",
    "    return {\n",
    "        \"plan_id\": plan_id,\n",
    "        \"plan_title\": plan_title,\n",
    "        \"plan_date\": plan_date,\n",
    "        \"plan_url\": url,\n",
    "        \"issues\": issues\n",
    "    }\n",
    "\n",
    "def load_existing_jsonl(path):\n",
    "    seen = set()\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    seen.add(data.get(\"plan_url\"))\n",
    "                except:\n",
    "                    continue\n",
    "    return seen\n",
    "\n",
    "def save_jsonl_entry(path, data):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    seen = load_existing_jsonl(OUTPUT_JSONL)\n",
    "\n",
    "    with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as out:\n",
    "        for url in tqdm(INPUT_URLS, desc=\"Scraping Level 1 plans\"):\n",
    "            if url in seen:\n",
    "                continue\n",
    "            try:\n",
    "                record = scrape_plan_page(url)\n",
    "                if record:\n",
    "                    save_jsonl_entry(OUTPUT_JSONL, record)\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error with {url}: {e}\")\n",
    "\n",
    "    print(f\"✅ Done. Results saved to {OUTPUT_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231db050",
   "metadata": {},
   "source": [
    "### Scraping Level 2 links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "028c573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Level 2 (full): 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Results saved to ../data/jsons/level2_issues_full.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://democracy.kent.gov.uk:9071\"\n",
    "OUTPUT_JSONL = \"../data/jsons/level2_issues_full.jsonl\"\n",
    "INPUT_URLS = [\n",
    "    \"https://democracy.kent.gov.uk:9071/mgIssueHistoryHome.aspx?IId=68660\",\n",
    "    \"https://democracy.kent.gov.uk:9071/mgIssueHistoryHome.aspx?IId=65693\"\n",
    "]\n",
    "\n",
    "def extract_issue_id_from_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parse_qs(parsed.query).get(\"IId\", [None])[0]\n",
    "\n",
    "def clean_narrative(soup):\n",
    "    content_div = soup.select_one(\"div.mgContent\")\n",
    "    if not content_div:\n",
    "        return \"\", []\n",
    "    paragraphs = content_div.find_all([\"p\", \"ul\", \"ol\", \"li\", \"h2\", \"h3\", \"strong\"])\n",
    "    texts = []\n",
    "    urls = set()\n",
    "    for tag in paragraphs:\n",
    "        text = tag.get_text(\" \", strip=True)\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "        for a in tag.find_all(\"a\", href=True):\n",
    "            urls.add(urljoin(BASE_URL, a[\"href\"]))\n",
    "    return \"\\n\\n\".join(texts), sorted(urls)\n",
    "\n",
    "def scrape_issue_history_page(url):\n",
    "    res = requests.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # Meta fields\n",
    "    meta_title = soup.find(\"meta\", attrs={\"name\": \"DC.title\"})\n",
    "    full_title = meta_title[\"content\"].strip() if meta_title else \"Unknown Title\"\n",
    "    if \" - \" in full_title:\n",
    "        parts = full_title.split(\" - \", 1)\n",
    "        decision_code = parts[0].strip()\n",
    "        title = parts[1].strip()\n",
    "    else:\n",
    "        decision_code = None\n",
    "        title = full_title\n",
    "\n",
    "    description = (soup.find(\"meta\", attrs={\"name\": \"DC.description\"}) or {}).get(\"content\", None)\n",
    "    created = (soup.find(\"meta\", attrs={\"name\": \"DC.date.created\"}) or {}).get(\"content\", None)\n",
    "    modified = (soup.find(\"meta\", attrs={\"name\": \"DC.date.modified\"}) or {}).get(\"content\", None)\n",
    "    subjects = [s[\"content\"] for s in soup.find_all(\"meta\", attrs={\"name\": \"DC.subject\"})]\n",
    "\n",
    "    # Narrative text and links\n",
    "    narrative_text, narrative_links = clean_narrative(soup)\n",
    "\n",
    "    # Decision links\n",
    "    decision_links = []\n",
    "    for a in soup.select(\"a[href*='ieDecisionDetails.aspx?Id=']\"):\n",
    "        href = a.get(\"href\")\n",
    "        full_url = urljoin(BASE_URL, href)\n",
    "        decision_id = parse_qs(urlparse(href).query).get(\"Id\", [None])[0]\n",
    "        if decision_id:\n",
    "            decision_links.append({\n",
    "                \"decision_id\": decision_id,\n",
    "                \"url\": full_url\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"issue_id\": extract_issue_id_from_url(url),\n",
    "        \"decision_code\": decision_code,\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"created\": created,\n",
    "        \"modified\": modified,\n",
    "        \"subjects\": subjects,\n",
    "        \"narrative\": narrative_text,\n",
    "        \"narrative_links\": narrative_links,\n",
    "        \"decision_links\": decision_links,\n",
    "        \"page_url\": url\n",
    "    }\n",
    "\n",
    "def load_existing_jsonl(path):\n",
    "    seen = set()\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    seen.add(data.get(\"page_url\"))\n",
    "                except:\n",
    "                    continue\n",
    "    return seen\n",
    "\n",
    "def save_jsonl_entry(path, data):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    seen = load_existing_jsonl(OUTPUT_JSONL)\n",
    "\n",
    "    with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as out:\n",
    "        for url in tqdm(INPUT_URLS, desc=\"Scraping Level 2 (full)\"):\n",
    "            if url in seen:\n",
    "                continue\n",
    "            try:\n",
    "                record = scrape_issue_history_page(url)\n",
    "                if record:\n",
    "                    save_jsonl_entry(OUTPUT_JSONL, record)\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error with {url}: {e}\")\n",
    "\n",
    "    print(f\"✅ Done. Results saved to {OUTPUT_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f97d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "585af4b5",
   "metadata": {},
   "source": [
    "### Level 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2d70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Level 4 issues:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Level 4 issues:  50%|█████     | 2/4 [00:00<00:00,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping Level 3 page: https://democracy.kent.gov.uk:9071/ieDecisionDetails.aspx?Id=2917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Level 4 issues: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Results saved to ../data/jsons/level4_issues.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_URLS = [\n",
    "    \"https://democracy.kent.gov.uk:9071/ieIssueDetails.aspx?IId=52995\",\n",
    "    \"https://democracy.kent.gov.uk:9071/ieIssueDetails.aspx?IId=52840\",\n",
    "    \"https://democracy.kent.gov.uk:9071/ieDecisionDetails.aspx?Id=2917\",\n",
    "    \"https://democracy.kent.gov.uk:9071/ieIssueDetails.aspx?IId=67560&PlanId=0&Opt=3#AI66632\"\n",
    "]\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_URL = \"https://democracy.kent.gov.uk:9071\"\n",
    "OUTPUT_JSONL = \"../data/jsons/level4_issues.jsonl\"\n",
    "\n",
    "\n",
    "def classify_doc_type(filename):\n",
    "    lowered = filename.lower()\n",
    "    if \"decision\" in lowered and \"record\" in lowered:\n",
    "        return \"Record of Decision\"\n",
    "    if \"eqia\" in lowered:\n",
    "        return \"Equality Impact Assessment\"\n",
    "    if \"appendix\" in lowered:\n",
    "        return \"Appendix\"\n",
    "    if \"report\" in lowered:\n",
    "        return \"Decision Report\"\n",
    "    return \"Other\"\n",
    "\n",
    "def extract_issue_id_from_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    return query.get(\"IId\", [None])[0]\n",
    "\n",
    "def scrape_issue_page(url):\n",
    "    if \"ieDecisionDetails.aspx\" in url:\n",
    "        print(f\"⚠️ Skipping Level 3 page: {url}\")\n",
    "        return None\n",
    "\n",
    "    res = requests.get(url, timeout=10)\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    # Title & decision code\n",
    "    meta_title = soup.find(\"meta\", attrs={\"name\": \"DC.title\"})\n",
    "    full_title = meta_title[\"content\"].strip() if meta_title else soup.title.text.strip()\n",
    "\n",
    "    if \" - \" in full_title:\n",
    "        parts = full_title.split(\" - \", 1)\n",
    "        decision_code = parts[0].strip()\n",
    "        title = parts[1].strip()\n",
    "    else:\n",
    "        decision_code = None\n",
    "        title = full_title\n",
    "\n",
    "    # Metadata\n",
    "    date_tag = soup.find(\"meta\", attrs={\"name\": \"DC.date.created\"})\n",
    "    created = date_tag[\"content\"] if date_tag else None\n",
    "    subject_tags = soup.find_all(\"meta\", attrs={\"name\": \"DC.subject\"})\n",
    "    subjects = [s[\"content\"] for s in subject_tags]\n",
    "\n",
    "    # PDF documents\n",
    "    documents = []\n",
    "    for a in soup.select(\"a[href*='/documents/']\"):\n",
    "        href = a.get(\"href\")\n",
    "        full_url = urljoin(BASE_URL, href)\n",
    "        filename = os.path.basename(href)\n",
    "        doc_type = classify_doc_type(filename)\n",
    "        documents.append({\n",
    "            \"filename\": filename,\n",
    "            \"doc_type\": doc_type,\n",
    "            \"url\": full_url\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"decision_code\": decision_code,\n",
    "        \"title\": title,\n",
    "        \"issue_id\": extract_issue_id_from_url(url),\n",
    "        \"page_url\": url,\n",
    "        \"created\": created,\n",
    "        \"subjects\": subjects,\n",
    "        \"documents\": documents\n",
    "    }\n",
    "\n",
    "def load_existing_jsonl(path):\n",
    "    seen = set()\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    seen.add(data.get(\"page_url\"))\n",
    "                except:\n",
    "                    continue\n",
    "    return seen\n",
    "\n",
    "def save_jsonl_entry(path, data):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    seen = load_existing_jsonl(OUTPUT_JSONL)\n",
    "\n",
    "    with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as out:\n",
    "        for url in tqdm(INPUT_URLS, desc=\"Scraping Level 4 issues\"):\n",
    "            if url in seen:\n",
    "                continue\n",
    "            try:\n",
    "                record = scrape_issue_page(url)\n",
    "                if record:\n",
    "                    save_jsonl_entry(OUTPUT_JSONL, record)\n",
    "                    time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error with {url}: {e}\")\n",
    "\n",
    "    print(f\"✅ Done. Results saved to {OUTPUT_JSONL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df0931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
