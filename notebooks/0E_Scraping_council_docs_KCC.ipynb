{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Sample of available keys and their counts (from first 100 entries):\n",
      "filename: 101\n",
      "path: 101\n",
      "type: 101\n",
      "committee: 101\n",
      "meeting_date: 101\n",
      "document_category: 101\n",
      "url: 101\n",
      "created: 101\n",
      "source_metadata_file: 101\n",
      "scraped: 101\n",
      "doc_id: 101\n",
      "title: 101\n",
      "author: 101\n",
      "subject: 101\n",
      "keywords: 101\n",
      "producer: 101\n",
      "creator: 101\n",
      "creation_date: 101\n",
      "mod_date: 101\n",
      "num_pages: 101\n",
      "hash: 101\n",
      "status: 101\n",
      "redirect_to: 4\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "from collections import Counter\n",
    "\n",
    "# Load the first 100 entries and collect all keys\n",
    "path = \"../data/document_metadata/raw_scraped_metadata.jsonl\"\n",
    "all_keys = []\n",
    "\n",
    "with jsonlines.open(path, \"r\") as reader:\n",
    "    for i, entry in enumerate(reader):\n",
    "        all_keys.extend(entry.keys())\n",
    "        if i >= 100:\n",
    "            break\n",
    "\n",
    "key_counts = Counter(all_keys)\n",
    "print(\"üß© Sample of available keys and their counts (from first 100 entries):\")\n",
    "for key, count in key_counts.items():\n",
    "    print(f\"{key}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08180e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a272bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fffae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d690672",
   "metadata": {},
   "source": [
    "### Code to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define path\n",
    "csv_path = \"/Users/lgfolder/Downloads/rtw.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Drop rows where required columns are missing (e.g., blank final row)\n",
    "df.dropna(subset=[\"topic\", \"mother_url\"], inplace=True)\n",
    "\n",
    "# Add 'status' column if missing\n",
    "if 'status' not in df.columns:\n",
    "    df['status'] = 'not_started'\n",
    "\n",
    "# Save cleaned file back (optional)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Display for review\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36434410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# === Load and prepare CSV ===\n",
    "csv_path = \"/Users/lgfolder/Downloads/rtw.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.dropna(subset=[\"topic\", \"mother_url\"], inplace=True)\n",
    "\n",
    "if 'status' not in df.columns:\n",
    "    df['status'] = 'not_started'\n",
    "\n",
    "# === Shared utilities ===\n",
    "base_url = \"https://democracy.kent.gov.uk/\"\n",
    "unknown_date_counter = 0\n",
    "def assign_unknown_folder():\n",
    "    global unknown_date_counter\n",
    "    unknown_date_counter += 1\n",
    "    return f\"unknown-{unknown_date_counter}\"\n",
    "\n",
    "def extract_meeting_date(soup):\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    match1 = re.search(r\"(?:Monday|Tuesday|Wednesday|Thursday|Friday),\\s+(\\d{1,2})(?:st|nd|rd|th)?(?:,)?\\s+([A-Za-z]+)(?:,)?\\s+(20\\d{2})\", text)\n",
    "    match2 = re.search(r\"held on (?:Monday|Tuesday|Wednesday|Thursday|Friday),\\s+(\\d{1,2})\\s+([A-Za-z]+)\\s+(20\\d{2})\", text)\n",
    "    for match in [match1, match2]:\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "            try:\n",
    "                return datetime.strptime(f\"{day} {month} {year}\", \"%d %B %Y\").strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def get_document_category(filename):\n",
    "    lower = filename.lower()\n",
    "    patterns = {\n",
    "        \"agenda_frontsheet\": [\"agenda\", \"front\"],\n",
    "        \"agenda\": [\"agenda\", \"additional agenda\", \"agenda item\"],\n",
    "        \"minutes\": [\"printed minutes\", \"cpp minutes\", \"minutes\"],\n",
    "        \"questions\": [\"questions\", \"answers\", \"q&a\"],\n",
    "        \"appendix\": [\"appendix\", \"annex\"],\n",
    "        \"motion\": [\"motion\", \"mtld\"],\n",
    "        \"amendment\": [\"amendment\"],\n",
    "        \"budget\": [\"budget\", \"revenue plan\"],\n",
    "        \"report\": [\"report\", \"covering report\", \"update\"],\n",
    "        \"decision_response\": [\"response\", \"decision\"],\n",
    "        \"strategy\": [\"strategy\"],\n",
    "        \"plan\": [\"plan\"],\n",
    "        \"policy\": [\"policy\", \"statement\"],\n",
    "        \"consultation\": [\"consultation\"],\n",
    "        \"performance\": [\"performance\", \"quarterly performance\", \"qpr\"],\n",
    "        \"terms_of_reference\": [\"terms of reference\", \"tor\"],\n",
    "        \"supporting_material\": [\"glossary\", \"note\"]\n",
    "    }\n",
    "    for category, keys in patterns.items():\n",
    "        if any(k in lower for k in keys):\n",
    "            return category\n",
    "    return \"other\"\n",
    "\n",
    "def download_pdf_and_record_metadata(pdf_url, destination_folder, meeting_date, seen_urls, topic):\n",
    "    if pdf_url in seen_urls:\n",
    "        return None\n",
    "    seen_urls.add(pdf_url)\n",
    "    filename = os.path.basename(urlparse(pdf_url).path)\n",
    "    original_name = filename\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(destination_folder, \"originals\", filename)):\n",
    "        root, ext = os.path.splitext(original_name)\n",
    "        filename = f\"{root}_{counter}{ext}\"\n",
    "        counter += 1\n",
    "    full_path = os.path.join(destination_folder, \"originals\", filename)\n",
    "    if not os.path.exists(full_path):\n",
    "        try:\n",
    "            r = requests.get(pdf_url, timeout=15)\n",
    "            r.raise_for_status()\n",
    "            os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "            with open(full_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Download error:\", e)\n",
    "            return None\n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"path\": f\"originals/{filename}\",\n",
    "        \"type\": \"pdf\",\n",
    "        \"committee\": topic,\n",
    "        \"meeting_date\": meeting_date,\n",
    "        \"document_category\": get_document_category(filename),\n",
    "        \"url\": pdf_url,\n",
    "        \"created\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# === Loop through all tasks ===\n",
    "for idx, row in df.iterrows():\n",
    "    topic = row[\"topic\"]\n",
    "    mother_url = row[\"mother_url\"]\n",
    "\n",
    "    if row.get(\"status\", \"not_started\") == \"completed\":\n",
    "        print(f\"‚úÖ Skipping already completed: {topic}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nüöÄ Starting: {topic}\")\n",
    "        df.at[idx, \"status\"] = \"in_progress\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        download_base = f\"/Users/lgfolder/Downloads/rtw_council/{topic}\"\n",
    "        resp = requests.get(mother_url)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        child_links = [urljoin(base_url, a[\"href\"]) for a in soup.select(\"a[href*='ieListDocuments.aspx']\")]\n",
    "        print(f\"  ‚û§ Found {len(child_links)} child pages\")\n",
    "\n",
    "        for child_url in child_links:\n",
    "            print(f\"  üìÑ Scraping child: {child_url}\")\n",
    "            resp = requests.get(child_url)\n",
    "            child_soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            meeting_date = extract_meeting_date(child_soup) or assign_unknown_folder()\n",
    "            meeting_folder = os.path.join(download_base, meeting_date)\n",
    "            os.makedirs(os.path.join(meeting_folder, \"originals\"), exist_ok=True)\n",
    "\n",
    "            seen_urls = set()\n",
    "            metadata = []\n",
    "\n",
    "            for a in child_soup.select(\"a[href$='.pdf']\"):\n",
    "                pdf_url = urljoin(base_url, a[\"href\"])\n",
    "                meta = download_pdf_and_record_metadata(pdf_url, meeting_folder, meeting_date, seen_urls, topic)\n",
    "                if meta:\n",
    "                    metadata.append(meta)\n",
    "\n",
    "            # Save metadata\n",
    "            if metadata:\n",
    "                metadata_path = os.path.join(meeting_folder, \"metadata.json\")\n",
    "                with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, indent=2)\n",
    "                print(f\"  ‚úÖ Metadata saved: {metadata_path}\")\n",
    "\n",
    "        df.at[idx, \"status\"] = \"completed\"\n",
    "        print(f\"üéâ Completed: {topic}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {topic} - {e}\")\n",
    "        df.at[idx, \"status\"] = \"failed\"\n",
    "\n",
    "    finally:\n",
    "        df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\n",
    "    \"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=113&Year=0\"  # full council\n",
    "]\n",
    "topic='full_council'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=115&Year=0\"\n",
    "               ]\n",
    "topic = 'cabinet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=128&Year=0\"\n",
    "               ]\n",
    "topic = 'pension'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=896&Year=0\"\n",
    "               ]\n",
    "topic = 'asc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6277fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=894&Year=0\"\n",
    "               ]\n",
    "topic = 'cype'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a66f5",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# === SETTINGS ===\n",
    "download_base = f\"/Users/lgfolder/Downloads/data scrape full 1 page only/{topic}\"\n",
    "base_url = \"https://democracy.kent.gov.uk/\"\n",
    "\n",
    "\n",
    "# === Unknown date tracker ===\n",
    "unknown_date_counter = 0\n",
    "def assign_unknown_folder():\n",
    "    global unknown_date_counter\n",
    "    unknown_date_counter += 1\n",
    "    return f\"unknown-{unknown_date_counter}\"\n",
    "\n",
    "# === FUNCTION: Extract meeting date from page ===\n",
    "def extract_meeting_date(soup):\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # Pattern 1: \"Thursday, 14 September 2023\"\n",
    "    match1 = re.search(\n",
    "        r\"(?:Thursday|Tuesday|Monday|Wednesday|Friday),\\s+(\\d{1,2})(?:st|nd|rd|th)?(?:,)?\\s+([A-Za-z]+)(?:,)?\\s+(20\\d{2})\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Pattern 2: \"Meeting of County Council held on Thursday, 17 October 2019 at 10.00 am\"\n",
    "    match2 = re.search(\n",
    "        r\"held on (?:Thursday|Tuesday|Monday|Wednesday|Friday),\\s+(\\d{1,2})\\s+([A-Za-z]+)\\s+(20\\d{2})\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    for match in [match1, match2]:\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "            raw_date = f\"{day} {month} {year}\"\n",
    "            try:\n",
    "                dt = datetime.strptime(raw_date, \"%d %B %Y\")\n",
    "                return dt.strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                print(f\"  ‚ö†Ô∏è Date parsing error: {raw_date}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# === FUNCTION: Categorise document based on filename ===\n",
    "def get_document_category(filename):\n",
    "    lower = filename.lower()\n",
    "\n",
    "    if \"agenda\" in lower and \"front\" in lower:\n",
    "        return \"agenda_frontsheet\"\n",
    "    elif \"agenda\" in lower or \"additional agenda\" in lower or \"agenda item\" in lower:\n",
    "        return \"agenda\"\n",
    "    elif \"printed minutes\" in lower or \"cpp minutes\" in lower or \"minutes of previous\" in lower:\n",
    "        return \"minutes\"\n",
    "    elif \"minutes\" in lower:\n",
    "        return \"minutes\"\n",
    "    elif \"questions put\" in lower or \"answers to questions\" in lower or \"q&a\" in lower:\n",
    "        return \"questions\"\n",
    "    elif \"appendix\" in lower or \"annex\" in lower:\n",
    "        return \"appendix\"\n",
    "    elif \"motion\" in lower or \"mtld\" in lower:\n",
    "        return \"motion\"\n",
    "    elif \"amendment\" in lower:\n",
    "        return \"amendment\"\n",
    "    elif \"budget\" in lower or \"revenue plan\" in lower:\n",
    "        return \"budget\"\n",
    "    elif \"report\" in lower or \"covering report\" in lower or \"update\" in lower:\n",
    "        return \"report\"\n",
    "    elif \"response\" in lower or \"decision\" in lower or \"record of decision\" in lower:\n",
    "        return \"decision_response\"\n",
    "    elif \"strategy\" in lower or \"investment strategy\" in lower or \"capital strategy\" in lower:\n",
    "        return \"strategy\"\n",
    "    elif \"plan\" in lower or \"local plan\" in lower or \"delivery plan\" in lower:\n",
    "        return \"plan\"\n",
    "    elif \"policy\" in lower or \"statement\" in lower:\n",
    "        return \"policy\"\n",
    "    elif \"consultation\" in lower:\n",
    "        return \"consultation\"\n",
    "    elif \"performance\" in lower or \"quarterly performance\" in lower or \"qpr\" in lower:\n",
    "        return \"performance\"\n",
    "    elif \"terms of reference\" in lower or \"tor\" in lower:\n",
    "        return \"terms_of_reference\"\n",
    "    elif \"glossary\" in lower or \"note\" in lower or \"you said we did\" in lower:\n",
    "        return \"supporting_material\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# === FUNCTION: Download file and collect metadata ===\n",
    "def download_pdf_and_record_metadata(pdf_url, destination_folder, meeting_date, seen_urls):\n",
    "    parsed = urlparse(pdf_url)\n",
    "\n",
    "    # Skip if hostname is missing or clearly internal\n",
    "    if not parsed.hostname or \"kent.gov.uk\" not in parsed.hostname:\n",
    "        print(f\"  ‚ö†Ô∏è Skipping invalid or internal link: {pdf_url}\")\n",
    "        return None\n",
    "\n",
    "    # Skip if URL already processed for this meeting\n",
    "    if pdf_url in seen_urls:\n",
    "        print(f\"    üîÅ Skipping duplicate URL: {pdf_url}\")\n",
    "        return None\n",
    "    seen_urls.add(pdf_url)\n",
    "\n",
    "    # Handle duplicate filenames by appending _1, _2, etc.\n",
    "    filename = os.path.basename(parsed.path)\n",
    "    original_name = filename\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(destination_folder, \"originals\", filename)):\n",
    "        filename_parts = os.path.splitext(original_name)\n",
    "        filename = f\"{filename_parts[0]}_{counter}{filename_parts[1]}\"\n",
    "        counter += 1\n",
    "\n",
    "    path_rel = os.path.join(\"originals\", filename)\n",
    "    full_path = os.path.join(destination_folder, path_rel)\n",
    "\n",
    "    # Download file if not already saved\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"    ‚¨áÔ∏è Downloading: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(pdf_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to download {pdf_url}: {e}\")\n",
    "            return None  # Skip this entry on error\n",
    "\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"    ‚è© Skipped (already exists): {filename}\")\n",
    "\n",
    "    # Return metadata\n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"path\": path_rel,\n",
    "        \"type\": \"pdf\",\n",
    "        \"committee\": topic,\n",
    "        \"meeting_date\": meeting_date,\n",
    "        \"document_category\": get_document_category(filename),\n",
    "        \"url\": pdf_url,\n",
    "        \"created\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# === MAIN SCRAPER LOOP ===\n",
    "for mother_url in mother_urls:\n",
    "    print(f\"\\nüîé Loading mother page: {mother_url}\")\n",
    "    resp = requests.get(mother_url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    child_links = [\n",
    "        urljoin(base_url, a[\"href\"])\n",
    "        for a in soup.select(\"a[href*='ieListDocuments.aspx']\")\n",
    "    ]\n",
    "\n",
    "    print(f\"  ‚û§ Found {len(child_links)} child pages.\")\n",
    "\n",
    "    for child_url in child_links:\n",
    "        print(f\"\\nüìÑ Scraping child page: {child_url}\")\n",
    "        child_resp = requests.get(child_url)\n",
    "        child_soup = BeautifulSoup(child_resp.text, \"html.parser\")\n",
    "\n",
    "        meeting_date = extract_meeting_date(child_soup)\n",
    "        if not meeting_date:\n",
    "            meeting_date = assign_unknown_folder()\n",
    "\n",
    "        meeting_folder = os.path.join(download_base, meeting_date)\n",
    "        originals_folder = os.path.join(meeting_folder, \"originals\")\n",
    "        os.makedirs(originals_folder, exist_ok=True)\n",
    "\n",
    "        metadata = []\n",
    "        seen_urls = set()  # Track URLs to avoid duplicates\n",
    "\n",
    "        for a in child_soup.select(\"a[href]\"):\n",
    "            href = a['href']\n",
    "            if '.pdf' in href.lower():\n",
    "                pdf_url = urljoin(base_url, href)\n",
    "                meta_entry = download_pdf_and_record_metadata(\n",
    "                    pdf_url, meeting_folder, meeting_date, seen_urls\n",
    "                )\n",
    "                if meta_entry:\n",
    "                    metadata.append(meta_entry)\n",
    "\n",
    "        # === GRANDCHILD LINKS ===\n",
    "        grandchild_links = [\n",
    "            urljoin(base_url, a[\"href\"])\n",
    "            for a in child_soup.select(\"a\")\n",
    "            if \"View the full list of documents\" in a.get_text()\n",
    "        ]\n",
    "\n",
    "        for g_url in grandchild_links:\n",
    "            print(f\"    ‚Ü™Ô∏è Scraping grandchild: {g_url}\")\n",
    "            g_resp = requests.get(g_url)\n",
    "            g_soup = BeautifulSoup(g_resp.text, \"html.parser\")\n",
    "\n",
    "            g_date = extract_meeting_date(g_soup) or assign_unknown_folder()\n",
    "            g_folder = os.path.join(download_base, g_date)\n",
    "            g_originals = os.path.join(g_folder, \"originals\")\n",
    "            os.makedirs(g_originals, exist_ok=True)\n",
    "\n",
    "            seen_urls = set()  # Reset for each grandchild\n",
    "\n",
    "            for a in g_soup.select(\"a[href]\"):\n",
    "                href = a['href']\n",
    "                if '.pdf' in href.lower():\n",
    "                    pdf_url = urljoin(base_url, href)\n",
    "                    meta_entry = download_pdf_and_record_metadata(\n",
    "                        pdf_url, g_folder, g_date, seen_urls\n",
    "                    )\n",
    "                    if meta_entry:\n",
    "                        metadata.append(meta_entry)\n",
    "\n",
    "        # === WRITE OR APPEND METADATA FILE (with deduplication) ===\n",
    "        metadata_path = os.path.join(meeting_folder, \"metadata.json\")\n",
    "        \n",
    "        # Load existing metadata if present\n",
    "        try:\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_metadata = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_metadata = []\n",
    "\n",
    "        # Remove None entries from new metadata\n",
    "        metadata = [entry for entry in metadata if entry is not None]\n",
    "\n",
    "        # Combine and deduplicate by URL\n",
    "        combined = {entry[\"url\"]: entry for entry in existing_metadata + metadata}\n",
    "        metadata = list(combined.values())\n",
    "\n",
    "        # Save updated metadata\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  üóÇÔ∏è Metadata updated at {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f83e1",
   "metadata": {},
   "source": [
    "### Correcting URLs in metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3096909",
   "metadata": {},
   "source": [
    "1.  Walks through every file matching ../data/council_documents/*/*/metadata.json\n",
    "2.  For each entry inside the metadata.json:\n",
    "\t‚Ä¢\tChecks if the \"url\" key exists and contains a space\n",
    "\t‚Ä¢\tIf so:\n",
    "\t‚Ä¢\tEncodes the URL safely using urllib.parse.quote\n",
    "\t‚Ä¢\tPreserves the query string (?T=9, etc.)\n",
    "\t‚Ä¢\tSets the updated value back to entry[\"url\"]\n",
    "3.\tIf any URLs were changed in that file:\n",
    "\t‚Ä¢\tIt overwrites the entire metadata.json with the updated entries using:json.dump(data, f, indent=2)\n",
    "4.\tLogs which files were updated.\n",
    "\n",
    "Careful: script modifies the file contents on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62282b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Set base path\n",
    "base_path = Path(\"../data/council_documents\")\n",
    "metadata_paths = list(base_path.glob(\"*/*/metadata.json\"))\n",
    "print(f\"üîç Found {len(metadata_paths)} metadata.json files\")\n",
    "\n",
    "for meta_file in metadata_paths:\n",
    "    try:\n",
    "        with meta_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        modified = False\n",
    "        for entry in data:\n",
    "            url = entry.get(\"url\")\n",
    "            if url and \" \" in url:\n",
    "                # Split to preserve any query string\n",
    "                if \"?\" in url:\n",
    "                    url_base, query = url.split(\"?\", 1)\n",
    "                    url_base_encoded = quote(url_base, safe=\"/:\")\n",
    "                    entry[\"url\"] = f\"{url_base_encoded}?{query}\"\n",
    "                else:\n",
    "                    entry[\"url\"] = quote(url, safe=\"/:\")\n",
    "                modified = True\n",
    "\n",
    "        if modified:\n",
    "            with meta_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            print(f\"‚úÖ Updated: {meta_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing {meta_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e708282",
   "metadata": {},
   "source": [
    "### Update and Overwrite \"path\" in All metadata.json Files (to make them unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb269a",
   "metadata": {},
   "source": [
    "What It Does:\n",
    "* Rebuilds the \"path\" field using: committee/meeting_date/originals/filename\n",
    "* Only overwrites the file if changes were made\n",
    "* Uses json.dump(..., indent=2, ensure_ascii=False) for human-readable formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9121b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "base_dir = Path(\"../data/council_documents\")\n",
    "metadata_files = list(base_dir.rglob(\"metadata.json\"))\n",
    "\n",
    "for meta_path in metadata_files:\n",
    "    with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    modified = False\n",
    "    for entry in metadata:\n",
    "        committee = entry.get(\"committee\")\n",
    "        date = entry.get(\"meeting_date\")\n",
    "        filename = entry.get(\"filename\")\n",
    "\n",
    "        if committee and date and filename:\n",
    "            new_path = f\"{committee}/{date}/originals/{filename}\"\n",
    "            if entry.get(\"path\") != new_path:\n",
    "                entry[\"path\"] = new_path\n",
    "                modified = True\n",
    "\n",
    "    if modified:\n",
    "        with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Updated: {meta_path}\")\n",
    "    else:\n",
    "        print(f\"‚ûñ No changes: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a1152",
   "metadata": {},
   "source": [
    "### Identifying and relocating Public Packs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relocate_public_packs.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from pathlib import Path\n",
    "\n",
    "def relocate_public_packs(base_folder=\"../data/council_documents\"):\n",
    "    base_path = Path(base_folder)\n",
    "\n",
    "    for committee in os.listdir(base_path):\n",
    "        committee_path = base_path / committee\n",
    "        if not committee_path.is_dir():\n",
    "            continue\n",
    "\n",
    "        for meeting_date in os.listdir(committee_path):\n",
    "            meeting_path = committee_path / meeting_date\n",
    "            originals_path = meeting_path / \"originals\"\n",
    "            public_packs_path = meeting_path / \"public_packs\"\n",
    "            metadata_path = meeting_path / \"metadata.json\"\n",
    "\n",
    "            if not originals_path.exists() or not metadata_path.exists():\n",
    "                continue\n",
    "\n",
    "            with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    metadata = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"‚ö†Ô∏è Skipping bad metadata: {metadata_path}\")\n",
    "                    continue\n",
    "\n",
    "            moved_filenames = []\n",
    "\n",
    "            for filename in os.listdir(originals_path):\n",
    "                if not filename.endswith(\".pdf\"):\n",
    "                    continue\n",
    "\n",
    "                file_path = originals_path / filename\n",
    "\n",
    "                try:\n",
    "                    reader = PdfReader(str(file_path))\n",
    "                    title = reader.metadata.get(\"/Title\", \"\")\n",
    "                    if \"(Public Pack)\" in title:\n",
    "                        os.makedirs(public_packs_path, exist_ok=True)\n",
    "                        dest_path = public_packs_path / filename\n",
    "                        shutil.move(str(file_path), str(dest_path))\n",
    "                        moved_filenames.append(filename)\n",
    "                        print(f\"‚úÖ Moved public pack: {file_path} ‚Üí {dest_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Could not process {file_path}: {e}\")\n",
    "\n",
    "            # Update metadata\n",
    "            modified = False\n",
    "            for entry in metadata:\n",
    "                if entry.get(\"filename\") in moved_filenames:\n",
    "                    entry[\"path\"] = f\"{committee}/{meeting_date}/public_packs/{entry['filename']}\"\n",
    "                    modified = True\n",
    "\n",
    "            if modified:\n",
    "                with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "                print(f\"üìù Metadata updated: {metadata_path}\")\n",
    "\n",
    "relocate_public_packs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef8ca1",
   "metadata": {},
   "source": [
    "### Hashing and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e43a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"../data/council_documents\")\n",
    "duplicates_dir = base_dir / \"duplicates\"\n",
    "duplicates_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def hash_pdf(file_path):\n",
    "    with file_path.open(\"rb\") as f:\n",
    "        return hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "def clean_filename_priority(name):\n",
    "    \"\"\"Return True if filename does NOT end with _1, _2, etc.\"\"\"\n",
    "    return not any(name.stem.endswith(f\"_{i}\") for i in range(1, 10))\n",
    "\n",
    "# Process each committee/meeting/originals folder\n",
    "for originals_dir in base_dir.rglob(\"originals\"):\n",
    "    if not originals_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    metadata_path = originals_dir.parent / \"metadata.json\"\n",
    "    if not metadata_path.exists():\n",
    "        print(f\"‚ö†Ô∏è No metadata found: {metadata_path}\")\n",
    "        continue\n",
    "\n",
    "    with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            metadata = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå Could not parse metadata: {metadata_path}\")\n",
    "            continue\n",
    "\n",
    "    # Build lookup: filename ‚ûù metadata entry\n",
    "    file_lookup = {entry[\"filename\"]: entry for entry in metadata if entry.get(\"filename\")}\n",
    "    updated_metadata = []\n",
    "\n",
    "    # Group files by hash\n",
    "    hash_groups = {}\n",
    "    for pdf_path in originals_dir.glob(\"*.pdf\"):\n",
    "        file_hash = hash_pdf(pdf_path)\n",
    "        hash_groups.setdefault(file_hash, []).append(pdf_path)\n",
    "\n",
    "    for file_hash, paths in hash_groups.items():\n",
    "        if len(paths) == 1:\n",
    "            # Keep as-is, just add hash\n",
    "            path = paths[0]\n",
    "            entry = file_lookup.get(path.name)\n",
    "            if entry:\n",
    "                entry[\"hash\"] = file_hash\n",
    "                updated_metadata.append(entry)\n",
    "            continue\n",
    "\n",
    "        # Choose file to keep\n",
    "        candidates = []\n",
    "        for path in paths:\n",
    "            entry = file_lookup.get(path.name)\n",
    "            if entry:\n",
    "                has_url = bool(entry.get(\"url\"))\n",
    "                is_clean = clean_filename_priority(path)\n",
    "                candidates.append((has_url, is_clean, path, entry))\n",
    "\n",
    "        candidates.sort(reverse=True)  # highest priority first\n",
    "        keep_path, keep_entry = candidates[0][2], candidates[0][3]\n",
    "        keep_entry[\"hash\"] = file_hash\n",
    "        updated_metadata.append(keep_entry)\n",
    "\n",
    "        # Remove the rest\n",
    "        for _, _, dupe_path, dupe_entry in candidates[1:]:\n",
    "            print(f\"üóë Removing duplicate: {dupe_path.name}\")\n",
    "            target_path = duplicates_dir / dupe_path.name\n",
    "            shutil.move(str(dupe_path), str(target_path))\n",
    "\n",
    "    # Save cleaned metadata\n",
    "    with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(updated_metadata, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚úÖ Updated metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4effd",
   "metadata": {},
   "source": [
    "### Diagnostic tool to consolidate data from underlying meeting level metadata generated during scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7432d16",
   "metadata": {},
   "source": [
    "This is a diagnostic or inspection tool ‚Äî useful for:\n",
    "* Finding missing URLs or metadata\n",
    "* Auditing document categories\n",
    "* Filtering or fixing records across the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path(\"../data/council_documents\")\n",
    "metadata_files = list(base_dir.rglob(\"metadata.json\"))\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for meta_path in metadata_files:\n",
    "    with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            entries = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    for entry in entries:\n",
    "        all_rows.append({\n",
    "            \"committee\": entry.get(\"committee\"),\n",
    "            \"meeting_date\": entry.get(\"meeting_date\"),\n",
    "            \"filename\": entry.get(\"filename\"),\n",
    "            \"document_category\": entry.get(\"document_category\"),\n",
    "            \"url\": entry.get(\"url\"),\n",
    "            \"path\": entry.get(\"path\"),\n",
    "            \"source_file\": str(meta_path)\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "metadata_df = pd.DataFrame(all_rows)\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d9883",
   "metadata": {},
   "source": [
    "### Auditing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9712de1e",
   "metadata": {},
   "source": [
    "The auditing and cleaning code below does 4 important things:\n",
    "* 1.\tChecks for missing metadata.\n",
    "* 2.\tDeduplicates entries in metadata.json.\n",
    "* 3.\tCompares recorded metadata with actual PDFs in the originals/ folder.\n",
    "* 4.\tDeletes truly empty folders and quarantines folders with discrepancies.\n",
    "\n",
    "Improvements:\n",
    "* Cleans up duplicate PDFs by URL and filename\n",
    "* Automatically renames suffix files to cleaner names\n",
    "* Keeps only one copy per duplicate group\n",
    "* Fully updates metadata.json:\n",
    "* Removes entries for deleted files\n",
    "* Updates filename/path for renamed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04f6788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deleted 0 files and renamed 0 files.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_folder = Path(\"../data/council_documents\")\n",
    "deleted_files = []\n",
    "renamed_files = []\n",
    "keep_set = set()\n",
    "\n",
    "for folder in base_folder.rglob(\"*/originals\"):\n",
    "    pdfs = list(folder.glob(\"*.pdf\"))\n",
    "    size_map = defaultdict(list)\n",
    "\n",
    "    metadata_path = folder.parent / \"metadata.json\"\n",
    "    url_lookup = {}\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            try:\n",
    "                metadata = json.load(f)\n",
    "                for entry in metadata:\n",
    "                    url_lookup[entry.get(\"filename\")] = entry.get(\"url\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for f in pdfs:\n",
    "        size_map[f.stat().st_size].append(f)\n",
    "\n",
    "    for size, files in size_map.items():\n",
    "        if len(files) > 1:\n",
    "            checked_pairs = set()\n",
    "            for i, row in enumerate(rows[:5], 1):\n",
    "                for f2 in files[i + 1:]:\n",
    "                    pair_key = tuple(sorted([f1.name, f2.name]))\n",
    "                    if pair_key in checked_pairs:\n",
    "                        continue\n",
    "                    checked_pairs.add(pair_key)\n",
    "\n",
    "                    name1, name2 = f1.name, f2.name\n",
    "                    ratio = SequenceMatcher(None, name1, name2).ratio()\n",
    "                    if ratio > 0.85:\n",
    "                        url1 = url_lookup.get(name1)\n",
    "                        url2 = url_lookup.get(name2)\n",
    "\n",
    "                        # Skip if either file is already marked to keep\n",
    "                        if f1 in keep_set or f2 in keep_set:\n",
    "                            continue\n",
    "\n",
    "                        def ends_with_suffix(name):\n",
    "                            return any(name.rstrip(\".pdf\").endswith(f\"_{n}\") for n in range(1, 10))\n",
    "\n",
    "                        # Case 1: One has a URL, the other doesn't\n",
    "                        if url1 and not url2:\n",
    "                            try:\n",
    "                                os.remove(f2)\n",
    "                                deleted_files.append((str(folder), name2))\n",
    "                                keep_set.add(f1)\n",
    "                                if ends_with_suffix(name1):\n",
    "                                    new_name = name1.replace(f\"_{name1.split('_')[-1]}\", \".pdf\")\n",
    "                                    new_path = f1.with_name(new_name)\n",
    "                                    if not new_path.exists():\n",
    "                                        try:\n",
    "                                            f1.rename(new_path)\n",
    "                                            renamed_files.append((str(folder), name1, new_name))\n",
    "                                            keep_set.discard(f1)\n",
    "                                            keep_set.add(new_path)\n",
    "                                        except FileNotFoundError:\n",
    "                                            pass\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "\n",
    "                        elif url2 and not url1:\n",
    "                            try:\n",
    "                                os.remove(f1)\n",
    "                                deleted_files.append((str(folder), name1))\n",
    "                                keep_set.add(f2)\n",
    "                                if ends_with_suffix(name2):\n",
    "                                    new_name = name2.replace(f\"_{name2.split('_')[-1]}\", \".pdf\")\n",
    "                                    new_path = f2.with_name(new_name)\n",
    "                                    if not new_path.exists():\n",
    "                                        try:\n",
    "                                            f2.rename(new_path)\n",
    "                                            renamed_files.append((str(folder), name2, new_name))\n",
    "                                            keep_set.discard(f2)\n",
    "                                            keep_set.add(new_path)\n",
    "                                        except FileNotFoundError:\n",
    "                                            pass\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "\n",
    "                        # Case 2: Both have URLs and they are equal ‚Üí keep cleaner name\n",
    "                        elif url1 == url2 and url1 is not None:\n",
    "                            if ends_with_suffix(name1):\n",
    "                                try:\n",
    "                                    os.remove(f1)\n",
    "                                    deleted_files.append((str(folder), name1))\n",
    "                                    keep_set.add(f2)\n",
    "                                except FileNotFoundError:\n",
    "                                    pass\n",
    "                            elif ends_with_suffix(name2):\n",
    "                                try:\n",
    "                                    os.remove(f2)\n",
    "                                    deleted_files.append((str(folder), name2))\n",
    "                                    keep_set.add(f1)\n",
    "                                except FileNotFoundError:\n",
    "                                    pass\n",
    "\n",
    "print(f\"‚úÖ Deleted {len(deleted_files)} files and renamed {len(renamed_files)} files.\")\n",
    "# pd.DataFrame(deleted_files, columns=[\"Folder\", \"Deleted File\"]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248d724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All folders are consistent with metadata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "base_path = \"/Users/lgfolder/github/council-assistant/data/council_documents/\"\n",
    "\n",
    "# === TRACKING ===\n",
    "quarantine_report = []\n",
    "\n",
    "def load_metadata(metadata_path):\n",
    "    try:\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def deduplicate_metadata(metadata):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for entry in metadata:\n",
    "        if isinstance(entry, dict):\n",
    "            fn = entry.get(\"filename\")\n",
    "            if fn and fn not in seen:\n",
    "                seen.add(fn)\n",
    "                deduped.append(entry)\n",
    "    return deduped\n",
    "\n",
    "def audit_folder(folder_name):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    if not os.path.isdir(folder_path) or folder_name == \"quarantine\":\n",
    "        return\n",
    "\n",
    "    metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
    "    originals_path = os.path.join(folder_path, \"originals\")\n",
    "\n",
    "    metadata = load_metadata(metadata_path)\n",
    "    metadata = deduplicate_metadata(metadata)\n",
    "\n",
    "    # Re-save deduplicated metadata if needed\n",
    "    if metadata:\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    metadata_filenames = {entry[\"filename\"] for entry in metadata if \"filename\" in entry}\n",
    "    actual_files = set()\n",
    "\n",
    "    if os.path.exists(originals_path):\n",
    "        for f in os.listdir(originals_path):\n",
    "            if f.lower().endswith(\".pdf\"):\n",
    "                actual_files.add(f)\n",
    "\n",
    "    # Report files that exist but are missing metadata entries\n",
    "    missing_from_metadata = actual_files - metadata_filenames\n",
    "    if missing_from_metadata:\n",
    "        quarantine_report.append({\n",
    "            \"folder\": folder_name,\n",
    "            \"missing_files\": sorted(missing_from_metadata)\n",
    "        })\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for folder in sorted(os.listdir(base_path)):\n",
    "    audit_folder(folder)\n",
    "\n",
    "# === REPORT ===\n",
    "if quarantine_report:\n",
    "    print(f\"\\nüìã Found {len(quarantine_report)} folders with missing metadata:\")\n",
    "    for entry in quarantine_report:\n",
    "        print(f\"\\nüìÖ {entry['folder']}\")\n",
    "        for fname in entry[\"missing_files\"]:\n",
    "            print(f\"   üîç {fname}\")\n",
    "else:\n",
    "    print(\"‚úÖ All folders are consistent with metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1b40d",
   "metadata": {},
   "source": [
    "### Enriching metadata with documenst from pdfs + Collecting all metadata from subfolders into one warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf097d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "import jsonlines\n",
    "from slugify import slugify  # pip install python-slugify\n",
    "\n",
    "# Define the root folder for PDFs and metadata\n",
    "base_dir = Path(\"../data/council_documents\")\n",
    "\n",
    "# Define and create the output folder for the new warehouse\n",
    "output_dir = Path(\"../data/document_metadata\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the output path for the final metadata file\n",
    "output_path = output_dir / \"document_metadata.jsonl\"\n",
    "# Gather all metadata.json files\n",
    "metadata_files = list(base_dir.rglob(\"metadata.json\"))\n",
    "all_metadata = []\n",
    "\n",
    "def hash_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            return hashlib.sha256(f.read()).hexdigest()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for meta_path in metadata_files:\n",
    "    with meta_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            entries = json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ö†Ô∏è Skipping: {meta_path}\")\n",
    "            continue\n",
    "\n",
    "    for entry in entries:\n",
    "        path = entry.get(\"path\")\n",
    "        if not path:\n",
    "            continue\n",
    "\n",
    "        pdf_path = base_dir / path\n",
    "        if not pdf_path.exists():\n",
    "            print(f\"‚ùå PDF not found: {pdf_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(str(pdf_path))\n",
    "            info = reader.metadata or {}\n",
    "            num_pages = len(reader.pages)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read PDF: {pdf_path.name} ‚Üí {e}\")\n",
    "            info = {}\n",
    "            num_pages = None\n",
    "\n",
    "        # Build full record\n",
    "        full_entry = dict(entry)\n",
    "\n",
    "        full_entry.update({\n",
    "            # doc_id will be added later\n",
    "            \"title\": info.get(\"/Title\", \"\"),\n",
    "            \"author\": info.get(\"/Author\", \"\"),\n",
    "            \"subject\": info.get(\"/Subject\", \"\"),\n",
    "            \"keywords\": info.get(\"/Keywords\", \"\"),\n",
    "            \"producer\": info.get(\"/Producer\", \"\"),\n",
    "            \"creator\": info.get(\"/Creator\", \"\"),\n",
    "            \"creation_date\": info.get(\"/CreationDate\", \"\"),\n",
    "            \"mod_date\": info.get(\"/ModDate\", \"\"),\n",
    "            \"num_pages\": num_pages,\n",
    "            \"file_size_kb\": round(pdf_path.stat().st_size / 1024, 1),\n",
    "            \"hash\": entry.get(\"hash\") or hash_pdf(pdf_path)\n",
    "        })\n",
    "\n",
    "        all_metadata.append(full_entry)\n",
    "\n",
    "with jsonlines.open(output_path, mode='w') as writer:\n",
    "    for record in all_metadata:\n",
    "        writer.write(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2b508",
   "metadata": {},
   "source": [
    "Open the metadata warehouse and review it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35def432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "records = []\n",
    "with jsonlines.open(Path(\"../data/document_metadata/document_metadata.jsonl\"), \"r\") as reader:\n",
    "    for obj in reader:\n",
    "        records.append(obj)\n",
    "\n",
    "doc_metadata_df = pd.DataFrame(records)\n",
    "#doc_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aab0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_metadata_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_metadata_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4babea",
   "metadata": {},
   "source": [
    "### Add doc_id to Document Metadata Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from slugify import slugify  # pip install python-slugify\n",
    "\n",
    "# Load document metadata\n",
    "metadata_path = Path(\"../data/document_metadata/document_metadata.jsonl\")\n",
    "records = []\n",
    "with jsonlines.open(metadata_path, \"r\") as reader:\n",
    "    for obj in reader:\n",
    "        records.append(obj)\n",
    "\n",
    "# Generate doc_id and update records\n",
    "for record in records:\n",
    "    committee = record.get(\"committee\")\n",
    "    date = record.get(\"meeting_date\")\n",
    "    filename = record.get(\"filename\")\n",
    "\n",
    "    if committee and date and filename:\n",
    "        doc_id = slugify(f\"{committee}__{date}__{filename}\")\n",
    "        record[\"doc_id\"] = doc_id\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing info for doc_id: {record.get('path')}\")\n",
    "\n",
    "# Overwrite with updated records\n",
    "with jsonlines.open(metadata_path, \"w\") as writer:\n",
    "    for record in records:\n",
    "        writer.write(record)\n",
    "\n",
    "print(f\"‚úÖ doc_id added to all valid records in: {metadata_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92a034",
   "metadata": {},
   "source": [
    "### Renaming categories of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load records\n",
    "metadata_path = Path(\"../data/document_metadata/document_metadata.jsonl\")\n",
    "records = []\n",
    "with jsonlines.open(metadata_path, \"r\") as reader:\n",
    "    for obj in reader:\n",
    "        records.append(obj)\n",
    "\n",
    "# Apply rules\n",
    "for entry in records:\n",
    "    fn = entry.get(\"filename\", \"\").lower()\n",
    "    title = entry.get(\"title\", \"\").lower()\n",
    "\n",
    "    # Rule 1: Based on filename\n",
    "    if \"agenda frontsheet\" in fn:\n",
    "        entry[\"document_category\"] = \"agenda_frontsheet\"\n",
    "\n",
    "    # Rule 2: Based on title\n",
    "    elif \"executive decision\" in title:\n",
    "        entry[\"document_category\"] = \"decision\"\n",
    "    elif \"agenda template\" in title:\n",
    "        entry[\"document_category\"] = \"agenda_frontsheet\"\n",
    "    elif \"record of decision\" in title:\n",
    "        entry[\"document_category\"] = \"decision\"\n",
    "    elif \"investment strategy\" in title:\n",
    "        entry[\"document_category\"] = \"strategy\"\n",
    "    elif \"plans\" in title:\n",
    "        entry[\"document_category\"] = \"plan\"\n",
    "    elif \"policy\" in title:\n",
    "        entry[\"document_category\"] = \"policy\"\n",
    "\n",
    "\n",
    "# Save updated jsonl\n",
    "with jsonlines.open(metadata_path, mode='w') as writer:\n",
    "    for entry in records:\n",
    "        writer.write(entry)\n",
    "\n",
    "print(f\"‚úÖ Updated and saved to: {metadata_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8e617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1718d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "with jsonlines.open(\"../data/processed_register/document_manifest.jsonl\", \"r\") as reader:\n",
    "    entries = list(reader)\n",
    "\n",
    "# How many are ready?\n",
    "ready = [e for e in entries if e.get(\"status\") == \"ready_for_embedding\"]\n",
    "print(f\"Found {len(ready)} entries with status = 'ready_for_embedding'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54053df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
