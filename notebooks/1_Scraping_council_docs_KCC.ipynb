{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d690672",
   "metadata": {},
   "source": [
    "### Code to scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcaadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\n",
    "    \"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=113&Year=0\"  # full council\n",
    "]\n",
    "topic='full_council'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1e1a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=115&Year=0\"\n",
    "               ]\n",
    "topic = 'cabinet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abc5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=128&Year=0\"\n",
    "               ]\n",
    "topic = 'pension'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_urls = [\"https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=896&Year=0\"\n",
    "               ]\n",
    "topic = 'asc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a66f5",
   "metadata": {},
   "source": [
    "### Final recovered version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67af309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Loading mother page: https://democracy.kent.gov.uk/ieListMeetings.aspx?CId=115&Year=0\n",
      "  ➤ Found 15 child pages.\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9471&Ver=4\n",
      "  🗂️ Metadata updated at /Users/lgfolder/github/council-assistant/data/council_documents/cabinet/2025-06-26/metadata.json\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9472&Ver=4\n",
      "  🗂️ Metadata updated at /Users/lgfolder/github/council-assistant/data/council_documents/cabinet/2025-06-05/metadata.json\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9761&Ver=4\n",
      "    ⬇️ Downloading: Agenda frontsheet 13th-Mar-2025 15.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: Public reports pack 13th-Mar-2025 15.00 Cabinet.pdf\n",
      "  🗂️ Metadata updated at /Users/lgfolder/github/council-assistant/data/council_documents/cabinet/2025-03-13/metadata.json\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9470&Ver=4\n",
      "    ⬇️ Downloading: Agenda frontsheet 04th-Mar-2025 10.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: Adoption of the Kent Minerals and Waste Local Plan 2024-2039 04th-Mar-2025 10.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: Minutes of Previous Meeting.pdf\n",
      "    ⬇️ Downloading: Cabinet Report 4 March 2025.pdf\n",
      "    ⬇️ Downloading: Appendix 1 Cabinet - Budget Update.pdf\n",
      "    ⬇️ Downloading: Cabinet QPR Q3 Cover.pdf\n",
      "    ⬇️ Downloading: QPR Q3 Cabinet appendix 1.pdf\n",
      "    ⬇️ Downloading: Scrutiny Committee - call-in Family Hubs Report to Cabinet - FINAL.pdf\n",
      "    ⬇️ Downloading: Adoption of the Kent Minerals and Waste Local Plan 2024-2039.pdf\n",
      "    ⬇️ Downloading: APPENDIX A - Planning Inspectors Report on the Examination of the KMWLP 2024-39.pdf\n",
      "    ⬇️ Downloading: APPENDIX B - ADOPTION of KMWLP 2024-39.pdf\n",
      "  🗂️ Metadata updated at /Users/lgfolder/github/council-assistant/data/council_documents/cabinet/2025-03-04/metadata.json\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9469&Ver=4\n",
      "    ⬇️ Downloading: Agenda frontsheet 30th-Jan-2025 10.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: 24-00096 - PROD 30th-Jan-2025 10.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: Item 7 - Supplementary - 2400109 - Transfer the 18-25 section of the Strengthening Independence Ser.pdf\n",
      "    ⬇️ Downloading: Printed minutes 30th-Jan-2025 10.00 Cabinet.pdf\n",
      "    ⬇️ Downloading: Minutes of the meeting held on 9th January 2025.pdf\n",
      "    ⬇️ Downloading: 24-00108 - Revenue and Capital Budget Monitoring Report November 2024-25.pdf\n",
      "    ⬇️ Downloading: 24-00108 - PROD.pdf\n",
      "    ⬇️ Downloading: 24-00108 - Finance Monitoring Report - Q3 - November 2024-25.pdf\n",
      "    ⬇️ Downloading: 24-00108 - Appendix - NOV 2024-25.pdf\n",
      "    ⬇️ Downloading: Draft Revenue Budget 2025-26 MTFP 2025-28 Draft Capital Programme 2025-35 and Treasury Management .pdf\n",
      "    ⬇️ Downloading: Draft Revenue Budget 2025-26 and 2025-28 MTFP and Draft Capital Programme 2025-35.pdf\n",
      "    ⬇️ Downloading: Appendix A Capital Investment Summary.pdf\n",
      "    ⬇️ Downloading: Appendix B Capital Investment by Directorate.pdf\n",
      "    ⬇️ Downloading: Appendix C - Potential Capital Projects.pdf\n",
      "    ⬇️ Downloading: Appendix D 21.01.25.pdf\n",
      "    ⬇️ Downloading: 24-00109 - Transfer the 18-25 section of the Strengthening Independence Service - Report.pdf\n",
      "    ⬇️ Downloading: 24-00109 - Appendix A - PROD.pdf\n",
      "    ⬇️ Downloading: 24-00109 - Appendix C - DPIA Screening Tool outcome - Transitions.pdf\n",
      "    ⬇️ Downloading: Item 7 - Supplementary - 24-00109 - Transfer the 18-25 section of the Strengthening Independence Ser.pdf\n",
      "    ⬇️ Downloading: Item 7 - Supplementary - 24-00109 - Appendix B - EqIA Updated Jan 2025.pdf\n",
      "    ⬇️ Downloading: 24-00115 - Adoption of Integrated Care Strategy delivery plan - Report.pdf\n",
      "    ⬇️ Downloading: 24-00115 - PROD.pdf\n",
      "    ⬇️ Downloading: 24-00115 - Appendix A - Current Health wellbeing activity within KCC.pdf\n",
      "    ⬇️ Downloading: 24-00115 - Appendix B - KCC ICS priorities measures.pdf\n",
      "    ⬇️ Downloading: 24-00115 - EqIA.pdf\n",
      "    ⬇️ Downloading: 24-00096 - Commissioning Plan for Education Provision in Kent 2025-29 - Report.pdf\n",
      "    ⬇️ Downloading: 24-00096 - Commissioning Plan for Education Provision in Kent 2025-29.pdf\n",
      "    ⬇️ Downloading: 24-00096 - EQIA.pdf\n",
      "    ⬇️ Downloading: 24-00096 - PRoD.pdf\n",
      "    ↪️ Scraping grandchild: https://democracy.kent.gov.uk/mgAi.aspx?ID=67237#mgDocuments\n",
      "    ⬇️ Downloading: Draft Revenue Budget 2025-26 MTFP 2025-28 Draft Capital Programme 2025-35 and Treasury Management _1.pdf\n",
      "    ⬇️ Downloading: Draft Revenue Budget 2025-26 and 2025-28 MTFP and Draft Capital Programme 2025-35_1.pdf\n",
      "    ⬇️ Downloading: Appendix A Capital Investment Summary_1.pdf\n",
      "    ⬇️ Downloading: Appendix B Capital Investment by Directorate_1.pdf\n",
      "    ⬇️ Downloading: Appendix C - Potential Capital Projects_1.pdf\n",
      "    ⬇️ Downloading: Appendix D 21.01.25_1.pdf\n",
      "    ⬇️ Downloading: Appendix E 21.01.25 incl base adj update.pdf\n",
      "    ⬇️ Downloading: Appendix F List of Budget Templates.pdf\n",
      "    ⬇️ Downloading: Appendix G - Reserves Policy.pdf\n",
      "    ⬇️ Downloading: Appendix H - Assessment of FInancial Resilience.pdf\n",
      "    ⬇️ Downloading: Appendix I Budget Risk Register 22.01.25.pdf\n",
      "    ⬇️ Downloading: Appendix J - Government Grants in LGFS.pdf\n",
      "    ⬇️ Downloading: Appendix K - Economic and Fiscal Context.pdf\n",
      "    ⬇️ Downloading: Appendix L - Treasury Management Strategy 2025-26.pdf\n",
      "    ⬇️ Downloading: Appendix M - Investment Strategy.pdf\n",
      "    ⬇️ Downloading: Appendix N - Capital Strategy.pdf\n",
      "    ⬇️ Downloading: Appendix O - MRP.pdf\n",
      "    ⬇️ Downloading: Appendix P - Flexible Use of Capital Receipts Strategy.pdf\n",
      "  🗂️ Metadata updated at /Users/lgfolder/github/council-assistant/data/council_documents/cabinet/2025-01-30/metadata.json\n",
      "\n",
      "📄 Scraping child page: https://democracy.kent.gov.uk/ieListDocuments.aspx?CId=115&MId=9468&Ver=4\n",
      "    ⬇️ Downloading: Agenda frontsheet 09th-Jan-2025 14.00 Cabinet.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 179\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m href\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m    178\u001b[0m     pdf_url \u001b[38;5;241m=\u001b[39m urljoin(base_url, href)\n\u001b[0;32m--> 179\u001b[0m     meta_entry \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_pdf_and_record_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeeting_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeeting_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseen_urls\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m meta_entry:\n\u001b[1;32m    183\u001b[0m         metadata\u001b[38;5;241m.\u001b[39mappend(meta_entry)\n",
      "Cell \u001b[0;32mIn[41], line 123\u001b[0m, in \u001b[0;36mdownload_pdf_and_record_metadata\u001b[0;34m(pdf_url, destination_folder, meeting_date, seen_urls)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    ⬇️ Downloading: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/sessions.py:745\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 745\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "# === SETTINGS ===\n",
    "download_base = f\"/Users/lgfolder/github/council-assistant/data/council_documents/{topic}\"\n",
    "base_url = \"https://democracy.kent.gov.uk/\"\n",
    "\n",
    "\n",
    "# === Unknown date tracker ===\n",
    "unknown_date_counter = 0\n",
    "def assign_unknown_folder():\n",
    "    global unknown_date_counter\n",
    "    unknown_date_counter += 1\n",
    "    return f\"unknown-{unknown_date_counter}\"\n",
    "\n",
    "# === FUNCTION: Extract meeting date from page ===\n",
    "def extract_meeting_date(soup):\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # Pattern 1: \"Thursday, 14 September 2023\"\n",
    "    match1 = re.search(\n",
    "        r\"(?:Thursday|Tuesday|Monday|Wednesday|Friday),\\s+(\\d{1,2})(?:st|nd|rd|th)?(?:,)?\\s+([A-Za-z]+)(?:,)?\\s+(20\\d{2})\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    # Pattern 2: \"Meeting of County Council held on Thursday, 17 October 2019 at 10.00 am\"\n",
    "    match2 = re.search(\n",
    "        r\"held on (?:Thursday|Tuesday|Monday|Wednesday|Friday),\\s+(\\d{1,2})\\s+([A-Za-z]+)\\s+(20\\d{2})\",\n",
    "        text\n",
    "    )\n",
    "\n",
    "    for match in [match1, match2]:\n",
    "        if match:\n",
    "            day, month, year = match.groups()\n",
    "            raw_date = f\"{day} {month} {year}\"\n",
    "            try:\n",
    "                dt = datetime.strptime(raw_date, \"%d %B %Y\")\n",
    "                return dt.strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                print(f\"  ⚠️ Date parsing error: {raw_date}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# === FUNCTION: Categorise document based on filename ===\n",
    "def get_document_category(filename):\n",
    "    lower = filename.lower()\n",
    "\n",
    "    if \"agenda\" in lower and \"front\" in lower:\n",
    "        return \"agenda_frontsheet\"\n",
    "    elif \"agenda\" in lower or \"additional agenda\" in lower or \"agenda item\" in lower:\n",
    "        return \"agenda\"\n",
    "    elif \"printed minutes\" in lower or \"cpp minutes\" in lower or \"minutes of previous\" in lower:\n",
    "        return \"minutes\"\n",
    "    elif \"minutes\" in lower:\n",
    "        return \"minutes\"\n",
    "    elif \"questions put\" in lower or \"answers to questions\" in lower or \"q&a\" in lower:\n",
    "        return \"questions\"\n",
    "    elif \"appendix\" in lower or \"annex\" in lower:\n",
    "        return \"appendix\"\n",
    "    elif \"motion\" in lower or \"mtld\" in lower:\n",
    "        return \"motion\"\n",
    "    elif \"amendment\" in lower:\n",
    "        return \"amendment\"\n",
    "    elif \"budget\" in lower or \"revenue plan\" in lower:\n",
    "        return \"budget\"\n",
    "    elif \"report\" in lower or \"covering report\" in lower or \"update\" in lower:\n",
    "        return \"report\"\n",
    "    elif \"response\" in lower or \"decision\" in lower or \"record of decision\" in lower:\n",
    "        return \"decision_response\"\n",
    "    elif \"strategy\" in lower or \"investment strategy\" in lower or \"capital strategy\" in lower:\n",
    "        return \"strategy\"\n",
    "    elif \"plan\" in lower or \"local plan\" in lower or \"delivery plan\" in lower:\n",
    "        return \"plan\"\n",
    "    elif \"policy\" in lower or \"statement\" in lower:\n",
    "        return \"policy\"\n",
    "    elif \"consultation\" in lower:\n",
    "        return \"consultation\"\n",
    "    elif \"performance\" in lower or \"quarterly performance\" in lower or \"qpr\" in lower:\n",
    "        return \"performance\"\n",
    "    elif \"terms of reference\" in lower or \"tor\" in lower:\n",
    "        return \"terms_of_reference\"\n",
    "    elif \"glossary\" in lower or \"note\" in lower or \"you said we did\" in lower:\n",
    "        return \"supporting_material\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# === FUNCTION: Download file and collect metadata ===\n",
    "def download_pdf_and_record_metadata(pdf_url, destination_folder, meeting_date, seen_urls):\n",
    "    parsed = urlparse(pdf_url)\n",
    "\n",
    "    # Skip if hostname is missing or clearly internal\n",
    "    if not parsed.hostname or \"kent.gov.uk\" not in parsed.hostname:\n",
    "        print(f\"  ⚠️ Skipping invalid or internal link: {pdf_url}\")\n",
    "        return None\n",
    "\n",
    "    # Skip if URL already processed for this meeting\n",
    "    if pdf_url in seen_urls:\n",
    "        print(f\"    🔁 Skipping duplicate URL: {pdf_url}\")\n",
    "        return None\n",
    "    seen_urls.add(pdf_url)\n",
    "\n",
    "    # Handle duplicate filenames by appending _1, _2, etc.\n",
    "    filename = os.path.basename(parsed.path)\n",
    "    original_name = filename\n",
    "    counter = 1\n",
    "    while os.path.exists(os.path.join(destination_folder, \"originals\", filename)):\n",
    "        filename_parts = os.path.splitext(original_name)\n",
    "        filename = f\"{filename_parts[0]}_{counter}{filename_parts[1]}\"\n",
    "        counter += 1\n",
    "\n",
    "    path_rel = os.path.join(\"originals\", filename)\n",
    "    full_path = os.path.join(destination_folder, path_rel)\n",
    "\n",
    "    # Download file if not already saved\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"    ⬇️ Downloading: {filename}\")\n",
    "        try:\n",
    "            response = requests.get(pdf_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to download {pdf_url}: {e}\")\n",
    "            return None  # Skip this entry on error\n",
    "\n",
    "        with open(full_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"    ⏩ Skipped (already exists): {filename}\")\n",
    "\n",
    "    # Return metadata\n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"path\": path_rel,\n",
    "        \"type\": \"pdf\",\n",
    "        \"committee\": topic,\n",
    "        \"meeting_date\": meeting_date,\n",
    "        \"document_category\": get_document_category(filename),\n",
    "        \"url\": pdf_url,\n",
    "        \"created\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# === MAIN SCRAPER LOOP ===\n",
    "for mother_url in mother_urls:\n",
    "    print(f\"\\n🔎 Loading mother page: {mother_url}\")\n",
    "    resp = requests.get(mother_url)\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    child_links = [\n",
    "        urljoin(base_url, a[\"href\"])\n",
    "        for a in soup.select(\"a[href*='ieListDocuments.aspx']\")\n",
    "    ]\n",
    "\n",
    "    print(f\"  ➤ Found {len(child_links)} child pages.\")\n",
    "\n",
    "    for child_url in child_links:\n",
    "        print(f\"\\n📄 Scraping child page: {child_url}\")\n",
    "        child_resp = requests.get(child_url)\n",
    "        child_soup = BeautifulSoup(child_resp.text, \"html.parser\")\n",
    "\n",
    "        meeting_date = extract_meeting_date(child_soup)\n",
    "        if not meeting_date:\n",
    "            meeting_date = assign_unknown_folder()\n",
    "\n",
    "        meeting_folder = os.path.join(download_base, meeting_date)\n",
    "        originals_folder = os.path.join(meeting_folder, \"originals\")\n",
    "        os.makedirs(originals_folder, exist_ok=True)\n",
    "\n",
    "        metadata = []\n",
    "        seen_urls = set()  # Track URLs to avoid duplicates\n",
    "\n",
    "        for a in child_soup.select(\"a[href]\"):\n",
    "            href = a['href']\n",
    "            if '.pdf' in href.lower():\n",
    "                pdf_url = urljoin(base_url, href)\n",
    "                meta_entry = download_pdf_and_record_metadata(\n",
    "                    pdf_url, meeting_folder, meeting_date, seen_urls\n",
    "                )\n",
    "                if meta_entry:\n",
    "                    metadata.append(meta_entry)\n",
    "\n",
    "        # === GRANDCHILD LINKS ===\n",
    "        grandchild_links = [\n",
    "            urljoin(base_url, a[\"href\"])\n",
    "            for a in child_soup.select(\"a\")\n",
    "            if \"View the full list of documents\" in a.get_text()\n",
    "        ]\n",
    "\n",
    "        for g_url in grandchild_links:\n",
    "            print(f\"    ↪️ Scraping grandchild: {g_url}\")\n",
    "            g_resp = requests.get(g_url)\n",
    "            g_soup = BeautifulSoup(g_resp.text, \"html.parser\")\n",
    "\n",
    "            g_date = extract_meeting_date(g_soup) or assign_unknown_folder()\n",
    "            g_folder = os.path.join(download_base, g_date)\n",
    "            g_originals = os.path.join(g_folder, \"originals\")\n",
    "            os.makedirs(g_originals, exist_ok=True)\n",
    "\n",
    "            seen_urls = set()  # Reset for each grandchild\n",
    "\n",
    "            for a in g_soup.select(\"a[href]\"):\n",
    "                href = a['href']\n",
    "                if '.pdf' in href.lower():\n",
    "                    pdf_url = urljoin(base_url, href)\n",
    "                    meta_entry = download_pdf_and_record_metadata(\n",
    "                        pdf_url, g_folder, g_date, seen_urls\n",
    "                    )\n",
    "                    if meta_entry:\n",
    "                        metadata.append(meta_entry)\n",
    "\n",
    "        # === WRITE OR APPEND METADATA FILE (with deduplication) ===\n",
    "        metadata_path = os.path.join(meeting_folder, \"metadata.json\")\n",
    "        \n",
    "        # Load existing metadata if present\n",
    "        try:\n",
    "            with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_metadata = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            existing_metadata = []\n",
    "\n",
    "        # Remove None entries from new metadata\n",
    "        metadata = [entry for entry in metadata if entry is not None]\n",
    "\n",
    "        # Combine and deduplicate by URL\n",
    "        combined = {entry[\"url\"]: entry for entry in existing_metadata + metadata}\n",
    "        metadata = list(combined.values())\n",
    "\n",
    "        # Save updated metadata\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  🗂️ Metadata updated at {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1faf8",
   "metadata": {},
   "source": [
    "### Auduting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df670fbe",
   "metadata": {},
   "source": [
    "Your auditing and cleaning code is already quite solid and well thought out. It does four important things:\n",
    "\t1.\tChecks for missing metadata.\n",
    "\t2.\tDeduplicates entries in metadata.json.\n",
    "\t3.\tCompares recorded metadata with actual PDFs in the originals/ folder.\n",
    "\t4.\tDeletes truly empty folders and quarantines folders with discrepancies.\n",
    "\n",
    "That said, here’s a refined and optimized version with improved structure, additional safety checks, and better separation of concerns. It avoids unnecessary memory usage and gives you a clean foundation for scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1be5d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Deleted 18 files and renamed 18 files.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_folder = Path(\"../data/council_documents\")\n",
    "deleted_files = []\n",
    "renamed_files = []\n",
    "keep_set = set()\n",
    "\n",
    "for folder in base_folder.rglob(\"*/originals\"):\n",
    "    pdfs = list(folder.glob(\"*.pdf\"))\n",
    "    size_map = defaultdict(list)\n",
    "\n",
    "    metadata_path = folder.parent / \"metadata.json\"\n",
    "    url_lookup = {}\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            try:\n",
    "                metadata = json.load(f)\n",
    "                for entry in metadata:\n",
    "                    url_lookup[entry.get(\"filename\")] = entry.get(\"url\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for f in pdfs:\n",
    "        size_map[f.stat().st_size].append(f)\n",
    "\n",
    "    for size, files in size_map.items():\n",
    "        if len(files) > 1:\n",
    "            checked_pairs = set()\n",
    "            for i, f1 in enumerate(files):\n",
    "                for f2 in files[i + 1:]:\n",
    "                    pair_key = tuple(sorted([f1.name, f2.name]))\n",
    "                    if pair_key in checked_pairs:\n",
    "                        continue\n",
    "                    checked_pairs.add(pair_key)\n",
    "\n",
    "                    name1, name2 = f1.name, f2.name\n",
    "                    ratio = SequenceMatcher(None, name1, name2).ratio()\n",
    "                    if ratio > 0.85:\n",
    "                        url1 = url_lookup.get(name1)\n",
    "                        url2 = url_lookup.get(name2)\n",
    "\n",
    "                        # Skip if either file is already marked to keep\n",
    "                        if f1 in keep_set or f2 in keep_set:\n",
    "                            continue\n",
    "\n",
    "                        def ends_with_suffix(name):\n",
    "                            return any(name.rstrip(\".pdf\").endswith(f\"_{n}\") for n in range(1, 10))\n",
    "\n",
    "                        # Case 1: One has a URL, the other doesn't\n",
    "                        if url1 and not url2:\n",
    "                            try:\n",
    "                                os.remove(f2)\n",
    "                                deleted_files.append((str(folder), name2))\n",
    "                                keep_set.add(f1)\n",
    "                                if ends_with_suffix(name1):\n",
    "                                    new_name = name1.replace(f\"_{name1.split('_')[-1]}\", \".pdf\")\n",
    "                                    new_path = f1.with_name(new_name)\n",
    "                                    if not new_path.exists():\n",
    "                                        try:\n",
    "                                            f1.rename(new_path)\n",
    "                                            renamed_files.append((str(folder), name1, new_name))\n",
    "                                            keep_set.discard(f1)\n",
    "                                            keep_set.add(new_path)\n",
    "                                        except FileNotFoundError:\n",
    "                                            pass\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "\n",
    "                        elif url2 and not url1:\n",
    "                            try:\n",
    "                                os.remove(f1)\n",
    "                                deleted_files.append((str(folder), name1))\n",
    "                                keep_set.add(f2)\n",
    "                                if ends_with_suffix(name2):\n",
    "                                    new_name = name2.replace(f\"_{name2.split('_')[-1]}\", \".pdf\")\n",
    "                                    new_path = f2.with_name(new_name)\n",
    "                                    if not new_path.exists():\n",
    "                                        try:\n",
    "                                            f2.rename(new_path)\n",
    "                                            renamed_files.append((str(folder), name2, new_name))\n",
    "                                            keep_set.discard(f2)\n",
    "                                            keep_set.add(new_path)\n",
    "                                        except FileNotFoundError:\n",
    "                                            pass\n",
    "                            except FileNotFoundError:\n",
    "                                pass\n",
    "\n",
    "                        # Case 2: Both have URLs and they are equal → keep cleaner name\n",
    "                        elif url1 == url2 and url1 is not None:\n",
    "                            if ends_with_suffix(name1):\n",
    "                                try:\n",
    "                                    os.remove(f1)\n",
    "                                    deleted_files.append((str(folder), name1))\n",
    "                                    keep_set.add(f2)\n",
    "                                except FileNotFoundError:\n",
    "                                    pass\n",
    "                            elif ends_with_suffix(name2):\n",
    "                                try:\n",
    "                                    os.remove(f2)\n",
    "                                    deleted_files.append((str(folder), name2))\n",
    "                                    keep_set.add(f1)\n",
    "                                except FileNotFoundError:\n",
    "                                    pass\n",
    "\n",
    "print(f\"✅ Deleted {len(deleted_files)} files and renamed {len(renamed_files)} files.\")\n",
    "# pd.DataFrame(deleted_files, columns=[\"Folder\", \"Deleted File\"]).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0cc3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All folders are consistent with metadata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "base_path = \"/Users/lgfolder/github/council-assistant/data/council_documents/\"\n",
    "\n",
    "# === TRACKING ===\n",
    "quarantine_report = []\n",
    "\n",
    "def load_metadata(metadata_path):\n",
    "    try:\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data if isinstance(data, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def deduplicate_metadata(metadata):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for entry in metadata:\n",
    "        if isinstance(entry, dict):\n",
    "            fn = entry.get(\"filename\")\n",
    "            if fn and fn not in seen:\n",
    "                seen.add(fn)\n",
    "                deduped.append(entry)\n",
    "    return deduped\n",
    "\n",
    "def audit_folder(folder_name):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    if not os.path.isdir(folder_path) or folder_name == \"quarantine\":\n",
    "        return\n",
    "\n",
    "    metadata_path = os.path.join(folder_path, \"metadata.json\")\n",
    "    originals_path = os.path.join(folder_path, \"originals\")\n",
    "\n",
    "    metadata = load_metadata(metadata_path)\n",
    "    metadata = deduplicate_metadata(metadata)\n",
    "\n",
    "    # Re-save deduplicated metadata if needed\n",
    "    if metadata:\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    metadata_filenames = {entry[\"filename\"] for entry in metadata if \"filename\" in entry}\n",
    "    actual_files = set()\n",
    "\n",
    "    if os.path.exists(originals_path):\n",
    "        for f in os.listdir(originals_path):\n",
    "            if f.lower().endswith(\".pdf\"):\n",
    "                actual_files.add(f)\n",
    "\n",
    "    # Report files that exist but are missing metadata entries\n",
    "    missing_from_metadata = actual_files - metadata_filenames\n",
    "    if missing_from_metadata:\n",
    "        quarantine_report.append({\n",
    "            \"folder\": folder_name,\n",
    "            \"missing_files\": sorted(missing_from_metadata)\n",
    "        })\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for folder in sorted(os.listdir(base_path)):\n",
    "    audit_folder(folder)\n",
    "\n",
    "# === REPORT ===\n",
    "if quarantine_report:\n",
    "    print(f\"\\n📋 Found {len(quarantine_report)} folders with missing metadata:\")\n",
    "    for entry in quarantine_report:\n",
    "        print(f\"\\n📅 {entry['folder']}\")\n",
    "        for fname in entry[\"missing_files\"]:\n",
    "            print(f\"   🔍 {fname}\")\n",
    "else:\n",
    "    print(\"✅ All folders are consistent with metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a367c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
