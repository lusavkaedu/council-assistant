{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6106dcf",
   "metadata": {},
   "source": [
    "## Scraping borough councils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2796027",
   "metadata": {},
   "source": [
    "### Scraping experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/lgfolder/Downloads/rtw.csv\")\n",
    "\n",
    "# Set for visited URLs to prevent duplicates\n",
    "visited = set()\n",
    "\n",
    "# Path to results file\n",
    "results_path = \"/Users/lgfolder/Downloads/pdf_results_rtw.json\"\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as f:\n",
    "        results_log = json.load(f)\n",
    "else:\n",
    "    results_log = []\n",
    "\n",
    "# Helper: Check if a URL points to a PDF\n",
    "def is_pdf(url):\n",
    "    return url.lower().endswith(\".pdf\")\n",
    "\n",
    "# Helper: Allow only ieListDocuments URLs with CId and MId (order-insensitive)\n",
    "def is_allowed_ieListDocuments_url(url):\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    parsed = urlparse(url)\n",
    "    if \"ieListDocuments.aspx\" not in parsed.path:\n",
    "        return False\n",
    "    query = parse_qs(parsed.query)\n",
    "    return \"CId\" in query and \"MId\" in query\n",
    "\n",
    "# Helper: Check if it's a valid starting page (meeting list)\n",
    "def is_valid_start_url(url):\n",
    "    return isinstance(url, str) and url.startswith(\"http\") and \"ieListMeetings.aspx\" in url\n",
    "\n",
    "# Crawl PDFs from a specific meeting document page\n",
    "def crawl_ieListDocuments_page(url, topic, source_url, depth=0, max_depth=2):\n",
    "    pdf_links = set()\n",
    "    if depth > max_depth or url in visited:\n",
    "        return pdf_links\n",
    "\n",
    "    if not is_allowed_ieListDocuments_url(url):\n",
    "        return pdf_links\n",
    "\n",
    "    visited.add(url)\n",
    "    print(f\"Crawling: {url} (depth={depth})\")\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            return pdf_links\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return pdf_links\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Try to extract committee, date, and time from a heading\n",
    "    committee = date = time_ = None\n",
    "    heading_text = soup.find(string=re.compile(r\" - .*\\d{1,2}.*\\d{4}\"))\n",
    "    if heading_text:\n",
    "        match = re.match(r\"(?P<committee>.+?) - .*?(?P<date>\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+,\\s+\\d{4})\\s+(?P<time>\\d{1,2}\\.\\d{2}\\s*[ap]m)\", heading_text.strip(), re.IGNORECASE)\n",
    "        if match:\n",
    "            committee = match.group(\"committee\").strip()\n",
    "            date = match.group(\"date\").strip()\n",
    "            time_ = match.group(\"time\").strip()\n",
    "\n",
    "    for a_tag in reversed(soup.find_all(\"a\", href=True)):\n",
    "        link = urljoin(url, a_tag['href'])\n",
    "        if is_pdf(link):\n",
    "            if link not in pdf_links:\n",
    "                pdf_links.add(link)\n",
    "                # Save immediately with metadata\n",
    "                results_log.append({\n",
    "                    \"topic\": topic,\n",
    "                    \"source_url\": source_url,\n",
    "                    \"page_url\": url,\n",
    "                    \"pdf_url\": link,\n",
    "                    \"committee\": committee,\n",
    "                    \"date\": date,\n",
    "                    \"time\": time_\n",
    "                })\n",
    "                with open(results_path, \"w\") as f:\n",
    "                    json.dump(results_log, f, indent=2)\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "# Top-level crawler: start from meeting list, extract child meeting document pages\n",
    "def crawl_from_meeting_list(start_url, topic):\n",
    "    meeting_pdf_links = set()\n",
    "\n",
    "    print(f\"Scanning meeting list: {start_url}\")\n",
    "    try:\n",
    "        resp = requests.get(start_url, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            return meeting_pdf_links\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch meeting list page {start_url}: {e}\")\n",
    "        return meeting_pdf_links\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(start_url, a_tag['href'])\n",
    "        if is_allowed_ieListDocuments_url(link):\n",
    "            meeting_pdf_links |= crawl_ieListDocuments_page(link, topic, start_url)\n",
    "            time.sleep(0.25)\n",
    "\n",
    "    return meeting_pdf_links\n",
    "\n",
    "# Run crawler for each topic\n",
    "pdf_map = {}\n",
    "for _, row in df.iterrows():\n",
    "    topic = row.get('topic')\n",
    "    start_url = row.get('mother_url')\n",
    "\n",
    "    if not is_valid_start_url(start_url):\n",
    "        print(f\"Skipping (not allowed): {start_url}\")\n",
    "        continue\n",
    "\n",
    "    visited.clear()\n",
    "    pdf_links = crawl_from_meeting_list(start_url, topic)\n",
    "    pdf_map[topic] = list(pdf_links)\n",
    "    print(f\"Saved {len(pdf_links)} PDFs for topic: {topic}\\n\")\n",
    "\n",
    "# The pdf_map now holds topic -> [list of PDF URLs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop duplicate PDF URLs, keeping the first occurrence\n",
    "df_deduped = df.drop_duplicates(subset=[\"pdf_url\"])\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content\n",
    "# Display the deduplicated DataFrame\n",
    "df_deduped  # or just df_deduped to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get unique page URLs\n",
    "unique_pages = df[\"page_url\"].drop_duplicates()\n",
    "\n",
    "# Display the count and the URLs\n",
    "#print(f\"Total unique page URLs: {len(unique_pages)}\")\n",
    "unique_pages.reset_index(drop=True)  # Nicely indexed for viewing\n",
    "unique_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a024e43",
   "metadata": {},
   "source": [
    "#### Downloading the texts and metadata from the pdfs without saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pdfplumber\n",
    "from io import BytesIO\n",
    "\n",
    "url = \"https://democracy.tunbridgewells.gov.uk/documents/s70214/15.%20Date%20of%20Next%20Meeting.pdf\"\n",
    "#url = \"https://democracy.tunbridgewells.gov.uk/documents/s76856/Appendix B - Appointments to Committees.pdf\"\n",
    "#url = \"https://democracy.tunbridgewells.gov.uk/documents/s76855/Appendix A Political balance and allocation of Committee seats.pdf\"\n",
    "#url = \"https://democracy.tunbridgewells.gov.uk/documents/s76866/10 Motion on Notice from Cllr Mobbs.pdf\"\n",
    "\n",
    "def extract_text_from_pdf_url(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        with pdfplumber.open(BytesIO(response.content)) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or ''\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Failed: {e}\"\n",
    "\n",
    "# Run it\n",
    "extract_text_from_pdf_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e543c",
   "metadata": {},
   "source": [
    "### Downloading pdf metadata from one url link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_all_pdf_metadata(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        content = response.content\n",
    "\n",
    "        reader = PdfReader(BytesIO(content))\n",
    "        raw_meta = reader.metadata or {}\n",
    "\n",
    "        # Convert keys to plain strings (e.g., \"/Author\" → \"Author\")\n",
    "        meta = {k.strip('/'): v for k, v in raw_meta.items()}\n",
    "\n",
    "        # Add general info\n",
    "        meta[\"Pages\"] = len(reader.pages)\n",
    "        meta[\"Encrypted\"] = reader.is_encrypted\n",
    "        meta[\"FileSizeBytes\"] = len(content)\n",
    "        meta[\"PDFHeader\"] = content[:8].decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "\n",
    "        # Page size (if available)\n",
    "        try:\n",
    "            box = reader.pages[0].mediabox\n",
    "            meta[\"PageWidth\"] = float(box.width)\n",
    "            meta[\"PageHeight\"] = float(box.height)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return meta\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage:\n",
    "metadata = extract_all_pdf_metadata(url)\n",
    "df_meta = pd.DataFrame([metadata])\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43283ec2",
   "metadata": {},
   "source": [
    "### Downloading metadata from all urls from the scraping results json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# Load results from your scraper\n",
    "with open(\"/Users/lgfolder/Downloads/pdf_results.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract unique PDF URLs\n",
    "pdf_urls = list({entry[\"pdf_url\"] for entry in data if entry.get(\"pdf_url\", \"\").endswith(\".pdf\")})\n",
    "\n",
    "def extract_all_pdf_metadata(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        content = response.content\n",
    "\n",
    "        reader = PdfReader(BytesIO(content))\n",
    "        raw_meta = reader.metadata or {}\n",
    "        meta = {k.strip('/'): v for k, v in raw_meta.items()}\n",
    "\n",
    "        # General info\n",
    "        meta[\"Pages\"] = len(reader.pages)\n",
    "        meta[\"Encrypted\"] = reader.is_encrypted\n",
    "        meta[\"FileSizeBytes\"] = len(content)\n",
    "        meta[\"PDFHeader\"] = content[:8].decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        meta[\"SourceURL\"] = url\n",
    "\n",
    "        # Page dimensions\n",
    "        try:\n",
    "            box = reader.pages[0].mediabox\n",
    "            meta[\"PageWidth\"] = float(box.width)\n",
    "            meta[\"PageHeight\"] = float(box.height)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return meta\n",
    "    except Exception as e:\n",
    "        return {\"SourceURL\": url, \"error\": str(e)}\n",
    "\n",
    "# Run metadata extraction for each PDF\n",
    "metadata_list = [extract_all_pdf_metadata(url) for url in tqdm(pdf_urls)]\n",
    "\n",
    "# Display as DataFrame\n",
    "df_meta = pd.DataFrame(metadata_list)\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d3041",
   "metadata": {},
   "source": [
    "### Scrape the full content of PDF files from links in JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83efc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, hashlib, requests, os\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# INPUT: Load results JSON\n",
    "with open(\"/Users/lgfolder/Downloads/pdf_results.json\", \"r\") as f:\n",
    "    scraped_records = json.load(f)\n",
    "\n",
    "# OUTPUT paths\n",
    "jsonl_path = \"/Users/lgfolder/Downloads/raw_scraped_metadata_rtw_test.jsonl\"\n",
    "id_register_path = \"/Users/lgfolder/Downloads/document_ids_rtw_test.json\"\n",
    "\n",
    "# Convert to lookup by URL\n",
    "scraped_by_url = {r[\"pdf_url\"]: r for r in scraped_records if \"pdf_url\" in r}\n",
    "\n",
    "# Load or initialize ID register\n",
    "if os.path.exists(id_register_path):\n",
    "    with open(id_register_path, \"r\") as f:\n",
    "        doc_id_register = json.load(f)\n",
    "else:\n",
    "    doc_id_register = {}\n",
    "\n",
    "# Track existing IDs to avoid duplication\n",
    "existing_ids = set(doc[\"id\"] for doc in doc_id_register.values())\n",
    "\n",
    "# Utility: Generate short hash-based ID\n",
    "def compute_doc_id(content):\n",
    "    short_hash = hashlib.sha256(content).hexdigest()[:8]\n",
    "    return f\"doc_{short_hash}\"\n",
    "\n",
    "# Utility: Clean string to avoid json serialization issues\n",
    "def clean_meta(meta):\n",
    "    return {k.strip(\"/\"): str(v) if v is not None else None for k, v in meta.items()}\n",
    "\n",
    "# Utility: Extract meeting date from title or subject\n",
    "def extract_meeting_date(text):\n",
    "    match = re.search(r\"(\\d{1,2})[\\-/](\\d{1,2})[\\-/](\\d{2,4})\", text)\n",
    "    if match:\n",
    "        d, m, y = match.groups()\n",
    "        if len(y) == 2:\n",
    "            y = '20' + y  # assume 21st century\n",
    "        try:\n",
    "            return datetime(int(y), int(m), int(d)).strftime(\"%Y-%m-%d\")\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Classify document type from title or URL\n",
    "def classify_doc_type(title, url):\n",
    "    title = (title or \"\").lower()\n",
    "    url = url.lower()\n",
    "    if \"agenda\" in title or \"agenda\" in url:\n",
    "        return \"Agenda\"\n",
    "    elif \"minute\" in title or \"minute\" in url:\n",
    "        return \"Minutes\"\n",
    "    elif \"report\" in title or \"report\" in url:\n",
    "        return \"Report\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Main extractor\n",
    "def extract_pdf_metadata(url):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        content = response.content\n",
    "\n",
    "        doc_id = compute_doc_id(content)\n",
    "        if doc_id in existing_ids:\n",
    "            return None, None  # already processed\n",
    "\n",
    "        reader = PdfReader(BytesIO(content))\n",
    "        raw_meta = clean_meta(reader.metadata or {})\n",
    "        text = ''.join((p.extract_text() or '') for p in reader.pages)\n",
    "\n",
    "        # Base record\n",
    "        record = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"source_url\": url,\n",
    "            \"title\": raw_meta.get(\"Title\"),\n",
    "            \"author\": raw_meta.get(\"Author\"),\n",
    "            \"creator\": raw_meta.get(\"Creator\"),\n",
    "            \"producer\": raw_meta.get(\"Producer\"),\n",
    "            \"created\": raw_meta.get(\"CreationDate\"),\n",
    "            \"modified\": raw_meta.get(\"ModDate\"),\n",
    "            \"subject\": raw_meta.get(\"Subject\"),\n",
    "            \"keywords\": raw_meta.get(\"Keywords\"),\n",
    "            \"pages\": len(reader.pages),\n",
    "            \"filesize_bytes\": len(content),\n",
    "            \"text\": text.strip(),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"avg_words_per_page\": round(len(text.split()) / max(1, len(reader.pages)), 2),\n",
    "            \"error\": None  # placeholder for error tracking\n",
    "        }\n",
    "\n",
    "        # Scraping step 1 integration\n",
    "        scraped = scraped_by_url.get(url, {})\n",
    "        record.update({\n",
    "            \"page_url\": scraped.get(\"page_url\"),\n",
    "            \"topic\": scraped.get(\"topic\"),\n",
    "            \"committee\": scraped.get(\"committee\"),\n",
    "            \"meeting_date\": scraped.get(\"date\"),\n",
    "            \"meeting_time\": scraped.get(\"time\")\n",
    "        })\n",
    "\n",
    "        # Fallback date extraction\n",
    "        if not record[\"meeting_date\"]:\n",
    "            record[\"meeting_date\"] = extract_meeting_date(record.get(\"title\", \"\") + record.get(\"subject\", \"\"))\n",
    "\n",
    "        # Document classification\n",
    "        record[\"document_type\"] = classify_doc_type(record.get(\"title\"), url)\n",
    "\n",
    "        return record, doc_id\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"source_url\": url, \"error\": str(e)}, None\n",
    "\n",
    "# Collect and process all unique PDF URLs\n",
    "pdf_urls = list({entry[\"pdf_url\"] for entry in scraped_records if entry.get(\"pdf_url\", \"\").endswith(\".pdf\")})\n",
    "\n",
    "with jsonlines.open(jsonl_path, mode=\"a\") as writer:\n",
    "    for url in tqdm(pdf_urls):\n",
    "        record, doc_id = extract_pdf_metadata(url)\n",
    "        if record and doc_id:\n",
    "            writer.write(record)\n",
    "            doc_id_register[url] = {\"id\": doc_id}\n",
    "        time.sleep(2.0)\n",
    "\n",
    "# Save document ID register\n",
    "with open(id_register_path, \"w\") as f:\n",
    "    json.dump(doc_id_register, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "\n",
    "# Path to your metadata file\n",
    "\n",
    "# Load all lines\n",
    "with jsonlines.open(jsonl_path) as reader:\n",
    "    records = list(reader)\n",
    "\n",
    "# Remove the 'text' field from each record\n",
    "for r in records:\n",
    "    r.pop(\"text\", None)\n",
    "\n",
    "# Create and display DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00183a60",
   "metadata": {},
   "source": [
    "### EDA of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed770dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jsonlines\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Load enriched metadata\n",
    "#jsonl_path = \"/Users/lgfolder/Downloads/raw_scraped_metadata_rtw_test_enriched.jsonl\"\n",
    "jsonl_path = '../data/meetings/meetings_metadata.jsonl'\n",
    "with jsonlines.open(jsonl_path) as reader:\n",
    "    records = list(reader)\n",
    "\n",
    "# Drop text field for performance\n",
    "for r in records:\n",
    "    r.pop(\"text\", None)\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5051657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Fix PDF metadata date format (e.g., D:20240313121419+00'00') ===\n",
    "def clean_pdf_date(date_str):\n",
    "    if not isinstance(date_str, str) or not date_str.startswith(\"D:\"):\n",
    "        return None\n",
    "    try:\n",
    "        match = re.match(r\"D:(\\d{4})(\\d{2})(\\d{2})(\\d{2})(\\d{2})(\\d{2})\", date_str)\n",
    "        if match:\n",
    "            y, m, d, h, mi, s = map(int, match.groups())\n",
    "            return datetime(y, m, d, h, mi, s)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "df[\"created_parsed\"] = df[\"created\"].apply(clean_pdf_date)\n",
    "df[\"year_month\"] = df[\"created_parsed\"].dt.to_period(\"M\")\n",
    "\n",
    "# === Parse meeting_date to YYYY-MM-DD ===\n",
    "df[\"meeting_date_parsed\"] = pd.to_datetime(df[\"meeting_date\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# === Additional prep ===\n",
    "df[\"author_clean\"] = df[\"author\"].fillna(\"Unknown\")\n",
    "df[\"text_length\"] = df[\"char_count\"]  # Already computed earlier\n",
    "df[\"pages\"] = pd.to_numeric(df[\"pages\"], errors=\"coerce\")\n",
    "\n",
    "# === Visualizations ===\n",
    "\n",
    "# 1. Creation date distribution\n",
    "if df[\"year_month\"].notna().any():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    df[\"year_month\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"PDFs by Creation Date (Year-Month)\")\n",
    "    plt.xlabel(\"Creation Date\")\n",
    "    plt.ylabel(\"Document Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Top 10 authors\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"author_clean\"].value_counts().head(10).plot(kind=\"bar\")\n",
    "plt.title(\"Top 10 Authors\")\n",
    "plt.xlabel(\"Author\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Text length distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"text_length\"].plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Distribution of Text Lengths\")\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Page count distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "df[\"pages\"].plot(kind=\"hist\", bins=20)\n",
    "plt.title(\"Distribution of Document Page Counts\")\n",
    "plt.xlabel(\"Pages\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Document type\n",
    "if \"document_type\" in df:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df[\"document_type\"].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(\"Document Types\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Committee counts\n",
    "if \"committee\" in df:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    df[\"committee\"].value_counts().head(10).plot(kind=\"bar\")\n",
    "    plt.title(\"Top 10 Committees\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. Numeric density\n",
    "if \"number_density\" in df:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df[\"number_density\"].plot(kind=\"hist\", bins=30)\n",
    "    plt.title(\"Numeric Density Distribution\")\n",
    "    plt.xlabel(\"Numbers per Word\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 8. Table-heavy flag\n",
    "if \"is_table_heavy\" in df:\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    df[\"is_table_heavy\"].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(\"Table-Heavy Documents\")\n",
    "    plt.xlabel(\"True / False\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c00f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by parsed meeting date\n",
    "if df[\"meeting_date_parsed\"].notna().any():\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    df[\"meeting_date_parsed\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"Distribution of Documents by Meeting Date\")\n",
    "    plt.xlabel(\"Meeting Date\")\n",
    "    plt.ylabel(\"Number of Documents\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No valid meeting_date found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49709490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"meeting_month\"] = df[\"meeting_date_parsed\"].dt.to_period(\"M\")\n",
    "df[\"meeting_month\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(10, 4))\n",
    "plt.title(\"Documents by Meeting Month\")\n",
    "plt.xlabel(\"Meeting Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d08e3c",
   "metadata": {},
   "source": [
    "### Maidstone (encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d53ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs, unquote\n",
    "import base64\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"/Users/lgfolder/Downloads/maidstone.csv\")\n",
    "\n",
    "# Set for visited URLs to prevent duplicates\n",
    "visited = set()\n",
    "\n",
    "# Path to results file\n",
    "results_path = \"/Users/lgfolder/Downloads/pdf_results_maidstone.json\"\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as f:\n",
    "        results_log = json.load(f)\n",
    "else:\n",
    "    results_log = []\n",
    "\n",
    "# Helper: Check if a URL points to a PDF\n",
    "def is_pdf(url):\n",
    "    return url.lower().endswith(\".pdf\")\n",
    "\n",
    "# Helper: Allow only ieListDocuments URLs with CId and MId (order-insensitive)\n",
    "def is_allowed_ieListDocuments_url(url):\n",
    "    if not isinstance(url, str):\n",
    "        return False\n",
    "    parsed = urlparse(url)\n",
    "    if \"ieListDocuments.aspx\" not in parsed.path:\n",
    "        return False\n",
    "    query = parse_qs(parsed.query)\n",
    "    return \"CId\" in query and \"MId\" in query\n",
    "\n",
    "# Helper: Check if it's a valid starting page (meeting list or embedded list)\n",
    "def is_valid_start_url(url):\n",
    "    return isinstance(url, str) and url.startswith(\"http\") and (\n",
    "        \"ieListMeetings.aspx\" in url or \"sq_content_src\" in url)\n",
    "\n",
    "# Decode Maidstone-style embedded links\n",
    "\n",
    "def extract_embedded_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    qs = parse_qs(parsed.query)\n",
    "    sq_encoded = qs.get(\"sq_content_src\", [None])[0]\n",
    "    if not sq_encoded:\n",
    "        return None\n",
    "    try:\n",
    "        decoded = base64.b64decode(sq_encoded[1:]).decode(\"utf-8\")  # strip +\n",
    "        real_url = parse_qs(decoded).get(\"url\", [None])[0]\n",
    "        return unquote(real_url) if real_url else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Crawl PDFs from a specific meeting document page\n",
    "def crawl_ieListDocuments_page(url, topic, source_url, depth=0, max_depth=2):\n",
    "    pdf_links = set()\n",
    "    if depth > max_depth or url in visited:\n",
    "        return pdf_links\n",
    "\n",
    "    if not is_allowed_ieListDocuments_url(url):\n",
    "        return pdf_links\n",
    "\n",
    "    visited.add(url)\n",
    "    print(f\"Crawling: {url} (depth={depth})\")\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            return pdf_links\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return pdf_links\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Try to extract committee, date, and time from a heading\n",
    "    committee = date = time_ = None\n",
    "    heading_text = soup.find(string=re.compile(r\" - .*\\d{1,2}.*\\d{4}\"))\n",
    "    if heading_text:\n",
    "        match = re.match(r\"(?P<committee>.+?) - .*?(?P<date>\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+,\\s+\\d{4})\\s+(?P<time>\\d{1,2}\\.\\d{2}\\s*[ap]m)\", heading_text.strip(), re.IGNORECASE)\n",
    "        if match:\n",
    "            committee = match.group(\"committee\").strip()\n",
    "            date = match.group(\"date\").strip()\n",
    "            time_ = match.group(\"time\").strip()\n",
    "\n",
    "    for a_tag in reversed(soup.find_all(\"a\", href=True)):\n",
    "        link = urljoin(url, a_tag['href'])\n",
    "        if is_pdf(link):\n",
    "            if link not in pdf_links:\n",
    "                pdf_links.add(link)\n",
    "                # Save immediately with metadata\n",
    "                results_log.append({\n",
    "                    \"topic\": topic,\n",
    "                    \"source_url\": source_url,\n",
    "                    \"page_url\": url,\n",
    "                    \"pdf_url\": link,\n",
    "                    \"committee\": committee,\n",
    "                    \"date\": date,\n",
    "                    \"time\": time_\n",
    "                })\n",
    "                with open(results_path, \"w\") as f:\n",
    "                    json.dump(results_log, f, indent=2)\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "# Top-level crawler: start from meeting list, extract child meeting document pages\n",
    "def crawl_from_meeting_list(start_url, topic):\n",
    "    meeting_pdf_links = set()\n",
    "\n",
    "    # Decode embedded URL if present\n",
    "    embedded_url = extract_embedded_url(start_url)\n",
    "    if embedded_url:\n",
    "        print(f\"Resolved embedded URL: {embedded_url}\")\n",
    "        start_url = embedded_url\n",
    "\n",
    "    print(f\"Scanning meeting list: {start_url}\")\n",
    "    try:\n",
    "        resp = requests.get(start_url, timeout=10)\n",
    "        if resp.status_code != 200:\n",
    "            return meeting_pdf_links\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch meeting list page {start_url}: {e}\")\n",
    "        return meeting_pdf_links\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        link = urljoin(start_url, a_tag['href'])\n",
    "        if is_allowed_ieListDocuments_url(link):\n",
    "            meeting_pdf_links |= crawl_ieListDocuments_page(link, topic, start_url)\n",
    "            time.sleep(0.25)\n",
    "\n",
    "    return meeting_pdf_links\n",
    "\n",
    "# Run crawler for each topic\n",
    "pdf_map = {}\n",
    "for _, row in df.iterrows():\n",
    "    topic = row.get('topic')\n",
    "    start_url = row.get('mother_url')\n",
    "\n",
    "    if not is_valid_start_url(start_url):\n",
    "        print(f\"Skipping (not allowed): {start_url}\")\n",
    "        continue\n",
    "\n",
    "    visited.clear()\n",
    "    pdf_links = crawl_from_meeting_list(start_url, topic)\n",
    "    pdf_map[topic] = list(pdf_links)\n",
    "    print(f\"Saved {len(pdf_links)} PDFs for topic: {topic}\\n\")\n",
    "\n",
    "# The pdf_map now holds topic -> [list of PDF URLs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d53077",
   "metadata": {},
   "source": [
    "## KCC Meeting metadata EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load JSONL file into a DataFrame\n",
    "file_path = '../data/meetings/meetings_metadata.jsonl'\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Show basic info\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f463b99f",
   "metadata": {},
   "source": [
    "#### Loading the metadata and examining the scraping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d17211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows\n",
    "#data.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b50dd",
   "metadata": {},
   "source": [
    "removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicated web_meeting_codes\n",
    "duplicate_codes = data[data['web_meeting_code'].duplicated(keep=False)]\n",
    "\n",
    "# Sort and select specific columns\n",
    "duplicate_codes = duplicate_codes.sort_values('web_meeting_code')[\n",
    "    ['web_meeting_code', 'scrape_timestamp', 'meeting_title', 'meeting_status', 'meeting_date']\n",
    "]\n",
    "duplicate_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b259891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by web_meeting_code, keeping the last entry\n",
    "data = data.sort_values('scrape_timestamp')  # Ensure correct ordering\n",
    "data = data.drop_duplicates(subset='web_meeting_code', keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data['web_meeting_code'].duplicated().sum() == 0  # Should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659f0ea",
   "metadata": {},
   "source": [
    "checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "print(\"Columns:\", data.columns.tolist())\n",
    "\n",
    "# Null values\n",
    "print(\"\\nMissing values per column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Unique values per column\n",
    "print(\"\\nUnique values per column:\")\n",
    "for col in data.columns:\n",
    "    try:\n",
    "        unique_count = data[col].nunique()\n",
    "        print(f\"{col}: {unique_count}\")\n",
    "    except TypeError:\n",
    "        print(f\"{col}: ❌ Cannot count unique values (unhashable type like list or dict)\")\n",
    "        \n",
    "# If 'meeting_date' exists and is string, convert to datetime\n",
    "if 'meeting_date' in data.columns:\n",
    "    data['meeting_date'] = pd.to_datetime(data['meeting_date'], errors='coerce')\n",
    "    print(\"\\nDate range:\", data['meeting_date'].min(), \"to\", data['meeting_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert meeting_date to datetime if not already\n",
    "data['meeting_date'] = pd.to_datetime(data['meeting_date'], errors='coerce')\n",
    "\n",
    "# Drop rows where meeting_date is NaT\n",
    "data = data.dropna(subset=['meeting_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = data['web_meeting_code'].dropna().astype(int)\n",
    "code_range = pd.Series(range(codes.min(), codes.max() + 1))\n",
    "\n",
    "missing_codes = code_range[~code_range.isin(codes)]\n",
    "print(f\"Missing codes: {len(missing_codes)}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "codes.plot(kind='hist', bins=100)\n",
    "plt.title('Distribution of Scraped web_meeting_code')\n",
    "plt.xlabel('web_meeting_code')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert to datetime\n",
    "data['scrape_timestamp'] = pd.to_datetime(data['scrape_timestamp'], errors='coerce')\n",
    "\n",
    "# Floor to minute\n",
    "data['scrape_minute'] = data['scrape_timestamp'].dt.floor('T')\n",
    "\n",
    "# Count scrapes per minute\n",
    "scrape_counts = data['scrape_minute'].value_counts().sort_index()\n",
    "\n",
    "# Create full range of minutes between first and last scrape\n",
    "full_range = pd.date_range(start=data['scrape_minute'].min(),\n",
    "                           end=data['scrape_minute'].max(),\n",
    "                           freq='T')\n",
    "\n",
    "# Reindex with full range, fill missing with 0\n",
    "scrape_counts_full = scrape_counts.reindex(full_range, fill_value=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "scrape_counts_full.plot()\n",
    "plt.title('Scraped Meetings per Minute (with Gaps)')\n",
    "plt.xlabel('Scrape Time (Minute)')\n",
    "plt.ylabel('Number of Meetings Scraped')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert and extract meeting_month\n",
    "data['meeting_date'] = pd.to_datetime(data['meeting_date'], errors='coerce')\n",
    "data['meeting_month'] = data['meeting_date'].dt.to_period('M')\n",
    "\n",
    "# Count meetings per month\n",
    "monthly_counts = data['meeting_month'].value_counts().sort_index()\n",
    "\n",
    "# Create full continuous monthly index\n",
    "full_index = pd.period_range(start=data['meeting_month'].min(),\n",
    "                             end=data['meeting_month'].max(),\n",
    "                             freq='M')\n",
    "\n",
    "# Reindex to ensure gaps are shown with 0s\n",
    "monthly_counts_full = monthly_counts.reindex(full_index, fill_value=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "monthly_counts_full.plot(kind='bar')\n",
    "plt.title('Meetings per Month (Including Gaps)')\n",
    "plt.xlabel('Meeting Month')\n",
    "plt.ylabel('Number of Meetings')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_months = data[data['meeting_month'].isna()]\n",
    "#missing_months.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c978c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_month_count = data['meeting_month'].isna().sum()\n",
    "print(f\"Missing meeting_month values: {missing_month_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = data[data['meeting_date'].isna()]\n",
    "missing_dates[['meeting_id', 'meeting_title', 'committee_name', 'meeting_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fields you consider essential\n",
    "essential_fields = ['meeting_title', 'meeting_status', 'committee_name', 'meeting_date', 'meeting_time', 'agenda_items']\n",
    "\n",
    "# Filter rows where web_meeting_code exists AND any essential field is missing\n",
    "incomplete_rows = data[\n",
    "    data['web_meeting_code'].notna() & \n",
    "    data[essential_fields].isnull().any(axis=1)\n",
    "]\n",
    "\n",
    "print(len(incomplete_rows))\n",
    "incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun = incomplete_rows['web_meeting_code'].to_list()\n",
    "rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc52d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure codes are integers and sorted\n",
    "codes = data['web_meeting_code'].dropna().astype(int)\n",
    "all_codes = pd.Series(range(codes.min(), codes.max() + 1))\n",
    "\n",
    "# Identify missing codes\n",
    "missing_codes = all_codes[~all_codes.isin(codes)]\n",
    "\n",
    "# Group into consecutive ranges\n",
    "gap_ranges = []\n",
    "if not missing_codes.empty:\n",
    "    start = prev = missing_codes.iloc[0]\n",
    "    for code in missing_codes[1:]:\n",
    "        if code == prev + 1:\n",
    "            prev = code\n",
    "        else:\n",
    "            gap_ranges.append((start, prev))\n",
    "            start = prev = code\n",
    "    gap_ranges.append((start, prev))  # add final group\n",
    "\n",
    "# Create DataFrame of missing ranges\n",
    "missing_df = pd.DataFrame(gap_ranges, columns=['missing_start', 'missing_end'])\n",
    "missing_df['missing_count'] = missing_df['missing_end'] - missing_df['missing_start'] + 1\n",
    "\n",
    "missing_df.sort_values(by=['missing_count'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure meeting_date is datetime\n",
    "data['meeting_date'] = pd.to_datetime(data['meeting_date'], errors='coerce')\n",
    "\n",
    "# Create a Year-Month string column\n",
    "data['year_month'] = data['meeting_date'].dt.to_period('M').astype(str)\n",
    "\n",
    "# Group by year_month and get min, max, and count of web_meeting_code\n",
    "grouped = data.groupby('year_month')['web_meeting_code'].agg(['min', 'max', 'count']).reset_index()\n",
    "\n",
    "# Sort by month\n",
    "grouped = grouped.sort_values('year_month')\n",
    "grouped.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26ef870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure datetime is parsed\n",
    "data['meeting_date'] = pd.to_datetime(data['meeting_date'], errors='coerce')\n",
    "\n",
    "# Extract numeric month (1–12) and month name\n",
    "data['month'] = data['meeting_date'].dt.month\n",
    "data['month_name'] = data['meeting_date'].dt.month_name()\n",
    "\n",
    "# Group by month number and count meetings\n",
    "monthly_pattern = data.groupby(['month', 'month_name']).size().reset_index(name='count')\n",
    "\n",
    "# Sort by calendar order\n",
    "monthly_pattern = monthly_pattern.sort_values('month')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(monthly_pattern['month_name'], monthly_pattern['count'], color='cornflowerblue')\n",
    "plt.title('Seasonal Meeting Pattern by Calendar Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Meetings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a87300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few non-empty agenda items\n",
    "data['agenda_items'].dropna().iloc[0:3].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a4567",
   "metadata": {},
   "source": [
    "#### Cleanup of junk pdf scraping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7dbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_item_title(title):\n",
    "    if isinstance(title, str):\n",
    "        return re.sub(r'\\s*PDF\\s*\\d+(\\.\\d+)?\\s*(KB|MB)', '', title, flags=re.IGNORECASE).strip()\n",
    "    return title\n",
    "\n",
    "def clean_agenda_items(items):\n",
    "    if isinstance(items, list):\n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and 'item_title' in item:\n",
    "                item['item_title'] = clean_item_title(item['item_title'])\n",
    "    return items\n",
    "\n",
    "# Apply cleaning to the full DataFrame\n",
    "data['agenda_items'] = data['agenda_items'].apply(clean_agenda_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42332bf1",
   "metadata": {},
   "source": [
    "#### Explore agendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ca1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Explode agenda_items so each item gets its own row\n",
    "agenda_data = data.explode('agenda_items').dropna(subset=['agenda_items']).copy()\n",
    "\n",
    "# Flatten the nested dictionaries into columns\n",
    "agenda_flat = pd.json_normalize(agenda_data['agenda_items'])\n",
    "\n",
    "# Add meeting context to each item\n",
    "agenda_flat['meeting_id'] = agenda_data['meeting_id'].values\n",
    "agenda_flat['committee_name'] = agenda_data['committee_name'].values\n",
    "agenda_flat['meeting_date'] = agenda_data['meeting_date'].values\n",
    "\n",
    "# Preview\n",
    "agenda_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cf301",
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_flat.groupby('meeting_id').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34965104",
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_flat['item_title'].str.lower().str.extract(r'(\\w+)')[0].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1bcc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of all pdf URLs\n",
    "all_pdfs = [url for sublist in agenda_flat['pdf_urls'] if isinstance(sublist, list) for url in sublist]\n",
    "\n",
    "# Convert to Series for easier analysis\n",
    "pdf_series = pd.Series(all_pdfs)\n",
    "\n",
    "# Count total and duplicate entries\n",
    "total_pdfs = pdf_series.size\n",
    "unique_pdfs = pdf_series.nunique()\n",
    "duplicate_pdfs = total_pdfs - unique_pdfs\n",
    "\n",
    "print(f\"Total PDF URLs: {total_pdfs}\")\n",
    "print(f\"Unique PDF URLs: {unique_pdfs}\")\n",
    "print(f\"Duplicate PDF URLs: {duplicate_pdfs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dec208",
   "metadata": {},
   "source": [
    "### New committee cleanup approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each committee_name\n",
    "committee_counts = data['committee_name'].value_counts(dropna=False)\n",
    "\n",
    "# Display result\n",
    "committee_counts.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from unique non-null committee names\n",
    "committee_series = data['committee_name'].dropna().unique()\n",
    "df_committee_parts = pd.DataFrame({'original_name': committee_series})\n",
    "\n",
    "# Adjust split logic\n",
    "df_committee_parts['first_part'] = df_committee_parts['original_name'].apply(\n",
    "    lambda x: x.rsplit(',', 1)[0] if ',' in x else None\n",
    ")\n",
    "df_committee_parts['last_part'] = df_committee_parts['original_name'].apply(\n",
    "    lambda x: x.rsplit(',', 1)[1].strip() if ',' in x else x\n",
    ")\n",
    "\n",
    "df_committee_parts.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21705447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the new committee name\n",
    "def extract_last_part(name):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return name.rsplit(',', 1)[1].strip() if ',' in name else name\n",
    "\n",
    "# Apply the transformation back to data, overwriting it\n",
    "data['committee_name'] = data['committee_name'].apply(extract_last_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect datatypes and nested objects\n",
    "print(data.dtypes)\n",
    "\n",
    "# Optional: check a sample problematic row\n",
    "sample = data.sample(1)\n",
    "print(sample.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbfad8",
   "metadata": {},
   "source": [
    "### New agenda cleanup approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac983ece",
   "metadata": {},
   "source": [
    "Cell 1: Clean PDF garbage from titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a68d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PDF_PATTERN = re.compile(r'\\s*PDF\\s*\\d+(\\.\\d+)?\\s*(KB|MB)', flags=re.IGNORECASE)\n",
    "\n",
    "agenda_flat['item_title'] = agenda_flat['item_title'].fillna('').apply(\n",
    "    lambda x: PDF_PATTERN.sub('', x).strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7466e0",
   "metadata": {},
   "source": [
    "Cell 2: Tag low-value items instead of dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ebdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex patterns to flag low-value or boilerplate items\n",
    "JUNK_PATTERNS = [\n",
    "    re.compile(r'(?i)\\bapologies\\b'),\n",
    "    re.compile(r'(?i)declaration[s]? of (interest|disclosable|inter)'),\n",
    "    re.compile(r'(?i)date of next meeting'),\n",
    "    re.compile(r'(?i)exempt items'),\n",
    "    re.compile(r'(?i)^work programme'),\n",
    "    re.compile(r'(?i)future meeting'),\n",
    "    re.compile(r'(?i)^introduction'),\n",
    "    re.compile(r'(?i)substitutes'),\n",
    "    re.compile(r'(?i)questions'),\n",
    "    re.compile(r'(?i)tributes'),\n",
    "    re.compile(r'(?i)election of'),\n",
    "]\n",
    "\n",
    "def is_junk(title):\n",
    "    title = str(title).lower().strip()\n",
    "    return any(p.search(title) for p in JUNK_PATTERNS)\n",
    "\n",
    "agenda_flat['is_junk'] = agenda_flat['item_title'].apply(is_junk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e4e7e",
   "metadata": {},
   "source": [
    "Cell 3: Assign fallback item numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8659ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing or blank item_numbers with zero-padded fallback numbers\n",
    "agenda_flat['item_number'] = agenda_flat['item_number'].fillna('')\n",
    "\n",
    "agenda_flat['item_number'] = agenda_flat.groupby('meeting_id')['item_number'].transform(\n",
    "    lambda x: [item if item.strip() else f\"{i+1:03d}\" for i, item in enumerate(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82359a5",
   "metadata": {},
   "source": [
    "Cell 4: Generate agenda_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b098ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number part and zero-pad to 3 digits\n",
    "agenda_flat['item_num_clean'] = agenda_flat['item_number'].str.extract(r'(\\d+)')[0].fillna('000').str.zfill(3)\n",
    "\n",
    "# Concatenate with meeting_id\n",
    "agenda_flat['agenda_id'] = agenda_flat['meeting_id'].astype(str) + '_' + agenda_flat['item_num_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4aed5",
   "metadata": {},
   "source": [
    "Reorder columns in agenda_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired leading columns\n",
    "lead_cols = ['agenda_id', 'item_num_clean']\n",
    "\n",
    "# Get the rest of the columns (preserving order but excluding the leads)\n",
    "other_cols = [col for col in agenda_flat.columns if col not in lead_cols]\n",
    "\n",
    "# Reassign agenda_flat with new column order\n",
    "agenda_flat = agenda_flat[lead_cols + other_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b32f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "agenda_flat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign machine reasable meeting time\n",
    "#agenda_flat['meeting_date_ts'] = agenda_flat['meeting_date'].astype('int64') // 1_000_000  # milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbf29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort agenda items chronologically and by item within each meeting\n",
    "agenda_flat = agenda_flat.sort_values(by=['meeting_date', 'agenda_id'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d291e7",
   "metadata": {},
   "source": [
    "Cell 5: Save to agenda_items.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../data/metadata/agenda_items.jsonl'\n",
    "agenda_flat.to_json(output_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"✅ Saved {len(agenda_flat)} agenda items to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76754b",
   "metadata": {},
   "source": [
    "### Committee and agenda cleaning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "# Constants and compiled patterns\n",
    "PDF_PATTERN = re.compile(r'\\s*PDF\\s*\\d+(\\.\\d+)?\\s*(KB|MB)', flags=re.IGNORECASE)\n",
    "MINUTES_PATTERN = re.compile(r'(?i)^minutes of the meeting|^minutes for')\n",
    "JUNK_PATTERNS = [\n",
    "    re.compile(r'(?i)apologi(es|s)'),\n",
    "    re.compile(r'(?i)declaration[s]? of (interest|disclosable|inter)'),\n",
    "    re.compile(r'(?i)date of next meeting'),\n",
    "    re.compile(r'(?i)exempt items'),\n",
    "    re.compile(r'(?i)minutes'),\n",
    "    re.compile(r'(?i)^work programme'),\n",
    "    re.compile(r'(?i)future meeting'),\n",
    "    re.compile(r'(?i)^introduction'),\n",
    "    re.compile(r'(?i)substitutes'),\n",
    "    re.compile(r'(?i)questions'),\n",
    "    re.compile(r'(?i)tributes'),\n",
    "    re.compile(r'(?i)election of')\n",
    "]\n",
    "\n",
    "# Committee normalization setup\n",
    "keyword_map = {\n",
    "    'county council': 'County Council',\n",
    "    'kent and medway police and crime panel': 'Kent and Medway Police and Crime Panel',\n",
    "    'select committee': 'Select Committee',\n",
    "    'member development': 'Member Development',\n",
    "    'regulation committee appeal': 'Regulation Committee Appeal Panel (Transport)',\n",
    "    'standing advisory council on religious': 'Standing Advisory Council on Religious Education (SACRE)',\n",
    "    'adult social care': 'Adult Social Care Cabinet Committee',\n",
    "    'pension': 'Pension Fund Committee',\n",
    "    'education': \"Children's, Young People and Education Cabinet Committee\",\n",
    "    'personnel': \"Personnel Committee\",\n",
    "    'regulation committee member panel': 'Regulation Committee',\n",
    "    \"children's social care and health cabinet committee\": \"Children's, Young People and Education Cabinet Committee\",\n",
    "    'environment & transport cabinet committee': \"Environment & Transport Cabinet Committee\",\n",
    "    'governance and audit committee': \"Governance and Audit Committee\",\n",
    "    'nhs joint': \"Kent and Medway NHS Joint Overview and Scrutiny Committee\",\n",
    "    'standards committee': \"Standards Committee\",\n",
    "    'wellbeing board': \"Kent Health and Wellbeing Board\",\n",
    "    'scrutiny committee': \"Scrutiny Committee\",\n",
    "    'governor appointments': \"Governor Appointments Panel\",\n",
    "    'regulation committee appeal panel': \"Regulation Committee Appeal Panel (Transport)\",\n",
    "    \"mental health guardianship\": \"Regulation Committee\"\n",
    "}\n",
    "\n",
    "@lru_cache(maxsize=500)\n",
    "def normalize_committee(name):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    name_lower = name.lower()\n",
    "    for keyword, canonical in keyword_map.items():\n",
    "        if keyword in name_lower:\n",
    "            return canonical\n",
    "    return name\n",
    "\n",
    "# Cleaning functions\n",
    "def clean_single_item(item):\n",
    "    if not isinstance(item, dict):\n",
    "        return item\n",
    "    return {\n",
    "        **item,\n",
    "        'item_title': PDF_PATTERN.sub('', item.get('item_title', '')).strip()\n",
    "    }\n",
    "\n",
    "def is_junk_item(item):\n",
    "    if not isinstance(item, dict):\n",
    "        return True\n",
    "    title = str(item.get('item_title', '')).lower()\n",
    "    return (\n",
    "        not title.strip() or\n",
    "        any(pattern.search(title) for pattern in JUNK_PATTERNS) or\n",
    "        MINUTES_PATTERN.search(title)\n",
    "    )\n",
    "\n",
    "def filter_junk_items(items):\n",
    "    if not isinstance(items, list):\n",
    "        return []\n",
    "    return [item for item in items if not is_junk_item(item)]\n",
    "\n",
    "def clean_data_chunk(chunk):\n",
    "    \"\"\"Helper function for parallel processing\"\"\"\n",
    "    return chunk.assign(agenda_items=chunk['agenda_items'].apply(filter_junk_items))\n",
    "\n",
    "# Main cleaning pipeline\n",
    "def clean_data(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    # Convert dates\n",
    "    df['scrape_timestamp'] = pd.to_datetime(df['scrape_timestamp'], errors='coerce')\n",
    "    df['meeting_date'] = pd.to_datetime(df['meeting_date'], errors='coerce')\n",
    "    \n",
    "    # Clean agenda items\n",
    "    df['agenda_items'] = df['agenda_items'].apply(\n",
    "        lambda x: [clean_single_item(i) for i in x] if isinstance(x, list) else x\n",
    "    )\n",
    "    \n",
    "    # Normalize committees\n",
    "    df['committee_name'] = df['committee_name'].apply(normalize_committee)\n",
    "    df = df[~df['committee_name'].str.contains('forum', case=False, na=False)]\n",
    "    \n",
    "    # Filter junk items (sequential version - removed parallel processing)\n",
    "    df['agenda_items'] = df['agenda_items'].apply(filter_junk_items)\n",
    "    \n",
    "    # Final filtering\n",
    "    df = df[df['agenda_items'].str.len() > 0]\n",
    "    valid_committees = df['committee_name'].dropna().unique()\n",
    "    df = df[df['committee_name'].isin(valid_committees)]\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# View agenda items function\n",
    "def get_committee_agenda_items(df, committee_name):\n",
    "    committee_meetings = df[\n",
    "        df['committee_name'].str.lower() == committee_name.lower()\n",
    "    ].copy()\n",
    "    \n",
    "    if committee_meetings.empty:\n",
    "        print(f\"No meetings found for committee: {committee_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    agenda_data = committee_meetings.explode('agenda_items').dropna(subset=['agenda_items'])\n",
    "    agenda_flat = pd.json_normalize(agenda_data['agenda_items'])\n",
    "    \n",
    "    # Add meeting context\n",
    "    context_cols = ['meeting_id', 'committee_name', 'meeting_date', 'meeting_title']\n",
    "    for col in context_cols:\n",
    "        agenda_flat[col] = agenda_data[col].values\n",
    "    \n",
    "    agenda_flat['num_pdfs'] = agenda_flat['pdf_urls'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    return agenda_flat.sort_values(['meeting_date', 'item_number']).reset_index(drop=True)\n",
    "\n",
    "# Execute pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    FILE_PATH = '../data/meetings/meetings_metadata.jsonl'\n",
    "    \n",
    "    # Clean the data\n",
    "    cleaned_data = clean_data(FILE_PATH)\n",
    "    \n",
    "    # Example usage - view Cabinet items\n",
    "    cabinet_items = get_committee_agenda_items(cleaned_data, \"Governance and Audit Committee\")\n",
    "    print(f\"Found {len(cabinet_items)} agenda items for Cabinet\")\n",
    "    \n",
    "    # Show available committees\n",
    "    print(\"\\nAvailable committees:\")\n",
    "    print(cleaned_data['committee_name'].unique())\n",
    "    \n",
    "    # Uncomment to view the results\n",
    "    #print(cabinet_items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinet_items.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb8b60",
   "metadata": {},
   "source": [
    "### Code to delete error rows in the meetings_metadata.json (if corrupted and have errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_path = '../data/meetings/meetings_metadata.jsonl'\n",
    "backup_path = '../data/meetings/meetings_metadata_backup.jsonl'\n",
    "\n",
    "# Step 1: Create backup copy\n",
    "shutil.copy(source_path, backup_path)\n",
    "\n",
    "# Step 2: Load data\n",
    "data = pd.read_json(source_path, lines=True)\n",
    "\n",
    "# Step 3: Filter out rows where 'error' is not NaN\n",
    "cleaned_data = data[data['error'].isna()]\n",
    "\n",
    "# Step 4: Overwrite original file with cleaned data\n",
    "#cleaned_data.to_json(source_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"Backup saved to: {backup_path}\")\n",
    "print(f\"Cleaned file saved to: {source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2dbe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7652df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
